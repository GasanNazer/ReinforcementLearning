{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cab62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines\n",
    "#Download and install ROMs\n",
    "#!gdown -q http://www.atarimania.com/roms/Roms.rar\n",
    "#!pip install -q unrar\n",
    "#!mkdir ./roms_atari\n",
    "#!unrar x Roms.rar ./roms_atari > /dev/null 2>&1\n",
    "#!python -m atari_py.import_roms ./roms_atari > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46855b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive\n",
    "import gym.wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3df1447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28ca1bd00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\") #make_atari\n",
    "obs = np.array(env.reset())\n",
    "print(obs.shape)\n",
    "plt.title(\"what the raw input looks like\")\n",
    "plt.imshow(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a1eda70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28dc4ca30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Obseravtions (wrapped)\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\") #make_atari(\"BreakoutNoFrameskip-v4\")\n",
    "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
    "obs = np.array(env.reset())\n",
    "\n",
    "print(obs.shape)\n",
    "plt.title(\"agent observation (4 frames)\")\n",
    "plt.imshow(obs.transpose([0, 2, 1]).reshape([env.observation_space.shape[0], -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07cd6998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "# Actions\n",
    "print(env.action_space)\n",
    "print(env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c8feb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28dd07a60>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rewards\n",
    "env.reset()\n",
    "rewards = []\n",
    "while True:\n",
    "    obs, rew, done, info = env.step(env.action_space.sample())\n",
    "    rewards.append(rew)\n",
    "    if done:\n",
    "        break\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3e53ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 84, 84, 4)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 21:36:55.524745: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-16 21:36:55.524966: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "num_actions = 4\n",
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(84, 84, 4,))\n",
    "\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "# The first model makes the predictions for Q-values which are used to make a action.\n",
    "model = create_q_model()\n",
    "# Target model\n",
    "model_target = create_q_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af88ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99  # Discount factor for past rewards\n",
    "\n",
    "# Setting epsilon decay parameters\n",
    "epsilon = 1.0  \n",
    "epsilon_max_1 = 1.0 \n",
    "epsilon_min_1 = 0.2  \n",
    "epsilon_max_2 = epsilon_min_1  \n",
    "epsilon_min_2 = 0.1\n",
    "epsilon_max_3 = epsilon_min_2  \n",
    "epsilon_min_3 = 0.02\n",
    "\n",
    "epsilon_interval_1 = (epsilon_max_1 - epsilon_min_1)  \n",
    "epsilon_interval_2 = (epsilon_max_2 - epsilon_min_2)  \n",
    "epsilon_interval_3 = (epsilon_max_3 - epsilon_min_3)  \n",
    "\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "\n",
    "# Maximum Replay Buffer volume\n",
    "max_memory_length = 190000\n",
    "\n",
    "# Size of batch taken from replay buffer\n",
    "batch_size = 32  \n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# Train the model after 20 actions\n",
    "update_after_actions = 20\n",
    "\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "\n",
    "# In the Deepmind paper they use RMSProp however then Adam optimizer improves training time\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9bdb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "#model_name = 'breakout_model_1'\n",
    "#path = F\"/content/gdrive/MyDrive/models/{model_name}\" \n",
    "#model.save(path)\n",
    "\n",
    "# Loading the model\n",
    "# model = tf.keras.models.load_model(path)\n",
    "def save_model(episode):\n",
    "    model_name = f'breakout_alpha_progress_episodes_{episode}.44'\n",
    "    path = f\"./{model_name}\"\n",
    "    model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcba66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 0.08 at episode 93, frame count 10000, epsilon 0.980, loss 0.00017\n",
      "running reward: 0.09 at episode 185, frame count 20000, epsilon 0.972, loss 0.00007\n",
      "running reward: 0.08 at episode 281, frame count 30000, epsilon 0.964, loss 0.01673\n",
      "running reward: 0.05 at episode 377, frame count 40000, epsilon 0.956, loss 0.01574\n",
      "running reward: 0.11 at episode 463, frame count 50000, epsilon 0.948, loss 0.00020\n",
      "running reward: 0.09 at episode 556, frame count 60000, epsilon 0.940, loss 0.02642\n",
      "running reward: 0.14 at episode 640, frame count 70000, epsilon 0.932, loss 0.01340\n",
      "running reward: 0.07 at episode 734, frame count 80000, epsilon 0.924, loss 0.00017\n",
      "running reward: 0.08 at episode 824, frame count 90000, epsilon 0.916, loss 0.00032\n",
      "running reward: 0.11 at episode 910, frame count 100000, epsilon 0.908, loss 0.01324\n",
      "running reward: 0.07 at episode 1003, frame count 110000, epsilon 0.900, loss 0.00009\n",
      "running reward: 0.08 at episode 1097, frame count 120000, epsilon 0.892, loss 0.00026\n",
      "running reward: 0.08 at episode 1190, frame count 130000, epsilon 0.884, loss 0.00055\n",
      "running reward: 0.13 at episode 1271, frame count 140000, epsilon 0.876, loss 0.00016\n",
      "running reward: 0.09 at episode 1360, frame count 150000, epsilon 0.868, loss 0.00866\n",
      "running reward: 0.09 at episode 1452, frame count 160000, epsilon 0.860, loss 0.00012\n",
      "running reward: 0.04 at episode 1551, frame count 170000, epsilon 0.852, loss 0.00923\n",
      "running reward: 0.10 at episode 1639, frame count 180000, epsilon 0.844, loss 0.00683\n",
      "running reward: 0.09 at episode 1729, frame count 190000, epsilon 0.836, loss 0.00034\n",
      "running reward: 0.09 at episode 1824, frame count 200000, epsilon 0.828, loss 0.00021\n",
      "running reward: 0.09 at episode 1915, frame count 210000, epsilon 0.820, loss 0.00012\n",
      "running reward: 0.06 at episode 2012, frame count 220000, epsilon 0.812, loss 0.00056\n",
      "running reward: 0.08 at episode 2106, frame count 230000, epsilon 0.804, loss 0.00008\n",
      "running reward: 0.13 at episode 2191, frame count 240000, epsilon 0.796, loss 0.00015\n",
      "running reward: 0.13 at episode 2279, frame count 250000, epsilon 0.788, loss 0.00059\n",
      "running reward: 0.11 at episode 2375, frame count 260000, epsilon 0.780, loss 0.00033\n",
      "running reward: 0.06 at episode 2473, frame count 270000, epsilon 0.772, loss 0.00108\n",
      "running reward: 0.06 at episode 2570, frame count 280000, epsilon 0.764, loss 0.00011\n",
      "running reward: 0.06 at episode 2665, frame count 290000, epsilon 0.756, loss 0.00084\n",
      "running reward: 0.09 at episode 2757, frame count 300000, epsilon 0.748, loss 0.00021\n",
      "running reward: 0.05 at episode 2856, frame count 310000, epsilon 0.740, loss 0.00046\n",
      "running reward: 0.13 at episode 2942, frame count 320000, epsilon 0.732, loss 0.00221\n",
      "running reward: 0.06 at episode 3039, frame count 330000, epsilon 0.724, loss 0.00036\n",
      "running reward: 0.12 at episode 3123, frame count 340000, epsilon 0.716, loss 0.00049\n",
      "running reward: 0.12 at episode 3213, frame count 350000, epsilon 0.708, loss 0.00053\n",
      "running reward: 0.08 at episode 3305, frame count 360000, epsilon 0.700, loss 0.00030\n",
      "running reward: 0.10 at episode 3394, frame count 370000, epsilon 0.692, loss 0.00041\n",
      "running reward: 0.14 at episode 3479, frame count 380000, epsilon 0.684, loss 0.00189\n",
      "running reward: 0.03 at episode 3578, frame count 390000, epsilon 0.676, loss 0.00131\n",
      "running reward: 0.08 at episode 3672, frame count 400000, epsilon 0.668, loss 0.00126\n",
      "running reward: 0.16 at episode 3755, frame count 410000, epsilon 0.660, loss 0.00442\n",
      "running reward: 0.25 at episode 3831, frame count 420000, epsilon 0.652, loss 0.00018\n",
      "running reward: 0.31 at episode 3907, frame count 430000, epsilon 0.644, loss 0.00012\n",
      "running reward: 0.13 at episode 3995, frame count 440000, epsilon 0.636, loss 0.00012\n",
      "running reward: 0.25 at episode 4068, frame count 450000, epsilon 0.628, loss 0.00011\n",
      "running reward: 0.27 at episode 4145, frame count 460000, epsilon 0.620, loss 0.00129\n",
      "running reward: 0.15 at episode 4234, frame count 470000, epsilon 0.612, loss 0.00023\n",
      "running reward: 0.18 at episode 4314, frame count 480000, epsilon 0.604, loss 0.00027\n",
      "running reward: 0.29 at episode 4383, frame count 490000, epsilon 0.596, loss 0.00023\n",
      "running reward: 0.27 at episode 4461, frame count 500000, epsilon 0.588, loss 0.00276\n",
      "running reward: 0.38 at episode 4532, frame count 510000, epsilon 0.580, loss 0.00262\n",
      "running reward: 0.10 at episode 4624, frame count 520000, epsilon 0.572, loss 0.00386\n",
      "running reward: 0.17 at episode 4709, frame count 530000, epsilon 0.564, loss 0.00287\n",
      "running reward: 0.23 at episode 4789, frame count 540000, epsilon 0.556, loss 0.00018\n",
      "running reward: 0.45 at episode 4849, frame count 550000, epsilon 0.548, loss 0.00055\n",
      "running reward: 0.30 at episode 4928, frame count 560000, epsilon 0.540, loss 0.00147\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./breakout_alpha_progress_episodes_5000.44/assets\n",
      "running reward: 0.28 at episode 5011, frame count 570000, epsilon 0.532, loss 0.00143\n",
      "running reward: 0.24 at episode 5085, frame count 580000, epsilon 0.524, loss 0.00047\n",
      "running reward: 0.37 at episode 5158, frame count 590000, epsilon 0.516, loss 0.00134\n",
      "running reward: 0.19 at episode 5241, frame count 600000, epsilon 0.508, loss 0.00189\n",
      "running reward: 0.22 at episode 5325, frame count 610000, epsilon 0.500, loss 0.00142\n",
      "running reward: 0.28 at episode 5399, frame count 620000, epsilon 0.492, loss 0.00195\n",
      "running reward: 0.19 at episode 5482, frame count 630000, epsilon 0.484, loss 0.00078\n",
      "running reward: 0.27 at episode 5555, frame count 640000, epsilon 0.476, loss 0.00142\n",
      "running reward: 0.36 at episode 5625, frame count 650000, epsilon 0.468, loss 0.00140\n",
      "running reward: 0.35 at episode 5696, frame count 660000, epsilon 0.460, loss 0.00899\n",
      "running reward: 0.24 at episode 5779, frame count 670000, epsilon 0.452, loss 0.00855\n",
      "running reward: 0.11 at episode 5869, frame count 680000, epsilon 0.444, loss 0.00023\n",
      "running reward: 0.18 at episode 5950, frame count 690000, epsilon 0.436, loss 0.00346\n",
      "running reward: 0.36 at episode 6014, frame count 700000, epsilon 0.428, loss 0.00267\n",
      "running reward: 0.31 at episode 6093, frame count 710000, epsilon 0.420, loss 0.00047\n",
      "running reward: 0.28 at episode 6168, frame count 720000, epsilon 0.412, loss 0.00112\n",
      "running reward: 0.32 at episode 6245, frame count 730000, epsilon 0.404, loss 0.00083\n",
      "running reward: 0.34 at episode 6314, frame count 740000, epsilon 0.396, loss 0.00156\n",
      "running reward: 0.35 at episode 6386, frame count 750000, epsilon 0.388, loss 0.00019\n",
      "running reward: 0.41 at episode 6453, frame count 760000, epsilon 0.380, loss 0.00054\n",
      "running reward: 0.43 at episode 6521, frame count 770000, epsilon 0.372, loss 0.00054\n",
      "running reward: 0.19 at episode 6604, frame count 780000, epsilon 0.364, loss 0.00064\n",
      "running reward: 0.38 at episode 6673, frame count 790000, epsilon 0.356, loss 0.00333\n",
      "running reward: 0.47 at episode 6733, frame count 800000, epsilon 0.348, loss 0.00025\n",
      "running reward: 0.40 at episode 6799, frame count 810000, epsilon 0.340, loss 0.00085\n",
      "running reward: 0.63 at episode 6847, frame count 820000, epsilon 0.332, loss 0.00030\n",
      "running reward: 0.55 at episode 6913, frame count 830000, epsilon 0.324, loss 0.00123\n",
      "running reward: 0.26 at episode 6989, frame count 840000, epsilon 0.316, loss 0.00139\n",
      "running reward: 0.39 at episode 7058, frame count 850000, epsilon 0.308, loss 0.00060\n",
      "running reward: 0.71 at episode 7100, frame count 860000, epsilon 0.300, loss 0.00442\n",
      "running reward: 0.86 at episode 7155, frame count 870000, epsilon 0.292, loss 0.00116\n",
      "running reward: 0.72 at episode 7210, frame count 880000, epsilon 0.284, loss 0.00416\n",
      "running reward: 0.64 at episode 7266, frame count 890000, epsilon 0.276, loss 0.00096\n",
      "running reward: 0.64 at episode 7322, frame count 900000, epsilon 0.268, loss 0.00204\n",
      "running reward: 0.67 at episode 7376, frame count 910000, epsilon 0.260, loss 0.00094\n",
      "running reward: 0.78 at episode 7425, frame count 920000, epsilon 0.252, loss 0.00374\n",
      "running reward: 0.75 at episode 7483, frame count 930000, epsilon 0.244, loss 0.00093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 0.72 at episode 7533, frame count 940000, epsilon 0.236, loss 0.00128\n",
      "running reward: 0.65 at episode 7592, frame count 950000, epsilon 0.228, loss 0.00088\n",
      "running reward: 0.73 at episode 7634, frame count 960000, epsilon 0.220, loss 0.00029\n",
      "running reward: 0.86 at episode 7682, frame count 970000, epsilon 0.212, loss 0.00062\n",
      "running reward: 0.93 at episode 7726, frame count 980000, epsilon 0.204, loss 0.00457\n",
      "running reward: 0.89 at episode 7777, frame count 990000, epsilon 0.200, loss 0.00521\n",
      "running reward: 0.82 at episode 7827, frame count 1000000, epsilon 0.200, loss 0.01472\n",
      "running reward: 0.85 at episode 7876, frame count 1010000, epsilon 0.199, loss 0.00159\n",
      "running reward: 1.04 at episode 7914, frame count 1020000, epsilon 0.198, loss 0.00818\n",
      "running reward: 0.92 at episode 7969, frame count 1030000, epsilon 0.197, loss 0.00079\n",
      "running reward: 0.64 at episode 8027, frame count 1040000, epsilon 0.196, loss 0.00265\n",
      "running reward: 0.67 at episode 8078, frame count 1050000, epsilon 0.195, loss 0.00099\n",
      "running reward: 0.74 at episode 8133, frame count 1060000, epsilon 0.194, loss 0.00510\n",
      "running reward: 0.73 at episode 8186, frame count 1070000, epsilon 0.193, loss 0.00460\n",
      "running reward: 0.85 at episode 8233, frame count 1080000, epsilon 0.192, loss 0.00101\n",
      "running reward: 0.89 at episode 8282, frame count 1090000, epsilon 0.191, loss 0.00073\n",
      "running reward: 0.91 at episode 8328, frame count 1100000, epsilon 0.190, loss 0.00140\n",
      "running reward: 0.91 at episode 8380, frame count 1110000, epsilon 0.189, loss 0.01722\n",
      "running reward: 0.81 at episode 8432, frame count 1120000, epsilon 0.188, loss 0.00094\n",
      "running reward: 0.81 at episode 8481, frame count 1130000, epsilon 0.187, loss 0.00069\n",
      "running reward: 0.96 at episode 8524, frame count 1140000, epsilon 0.186, loss 0.00184\n",
      "running reward: 0.76 at episode 8586, frame count 1150000, epsilon 0.185, loss 0.00057\n",
      "running reward: 0.73 at episode 8630, frame count 1160000, epsilon 0.184, loss 0.02520\n",
      "running reward: 0.98 at episode 8671, frame count 1170000, epsilon 0.183, loss 0.00089\n",
      "running reward: 0.96 at episode 8720, frame count 1180000, epsilon 0.182, loss 0.00027\n",
      "running reward: 0.91 at episode 8766, frame count 1190000, epsilon 0.181, loss 0.00102\n",
      "running reward: 0.90 at episode 8816, frame count 1200000, epsilon 0.180, loss 0.00036\n",
      "running reward: 0.90 at episode 8861, frame count 1210000, epsilon 0.179, loss 0.00202\n",
      "running reward: 0.95 at episode 8908, frame count 1220000, epsilon 0.178, loss 0.00051\n",
      "running reward: 0.92 at episode 8956, frame count 1230000, epsilon 0.177, loss 0.00023\n",
      "running reward: 0.84 at episode 9006, frame count 1240000, epsilon 0.176, loss 0.00140\n",
      "running reward: 0.85 at episode 9055, frame count 1250000, epsilon 0.175, loss 0.00078\n",
      "running reward: 0.96 at episode 9097, frame count 1260000, epsilon 0.174, loss 0.00395\n",
      "running reward: 1.05 at episode 9139, frame count 1270000, epsilon 0.173, loss 0.00056\n",
      "running reward: 1.02 at episode 9186, frame count 1280000, epsilon 0.172, loss 0.00514\n",
      "running reward: 0.87 at episode 9236, frame count 1290000, epsilon 0.171, loss 0.00659\n",
      "running reward: 0.93 at episode 9277, frame count 1300000, epsilon 0.170, loss 0.00417\n",
      "running reward: 0.90 at episode 9326, frame count 1310000, epsilon 0.169, loss 0.00076\n",
      "running reward: 0.84 at episode 9375, frame count 1320000, epsilon 0.168, loss 0.00051\n",
      "running reward: 0.85 at episode 9423, frame count 1330000, epsilon 0.167, loss 0.00054\n",
      "running reward: 0.89 at episode 9468, frame count 1340000, epsilon 0.166, loss 0.00223\n",
      "running reward: 0.77 at episode 9524, frame count 1350000, epsilon 0.165, loss 0.00076\n",
      "running reward: 0.73 at episode 9576, frame count 1360000, epsilon 0.164, loss 0.00045\n",
      "running reward: 0.87 at episode 9621, frame count 1370000, epsilon 0.163, loss 0.00317\n",
      "running reward: 0.95 at episode 9665, frame count 1380000, epsilon 0.162, loss 0.00050\n",
      "running reward: 1.11 at episode 9704, frame count 1390000, epsilon 0.161, loss 0.00178\n",
      "running reward: 1.06 at episode 9750, frame count 1400000, epsilon 0.160, loss 0.00227\n",
      "running reward: 1.02 at episode 9795, frame count 1410000, epsilon 0.159, loss 0.00470\n",
      "running reward: 1.02 at episode 9836, frame count 1420000, epsilon 0.158, loss 0.00038\n",
      "running reward: 1.12 at episode 9873, frame count 1430000, epsilon 0.157, loss 0.00159\n",
      "running reward: 1.18 at episode 9914, frame count 1440000, epsilon 0.156, loss 0.00258\n",
      "running reward: 1.11 at episode 9957, frame count 1450000, epsilon 0.155, loss 0.00050\n",
      "running reward: 1.17 at episode 9996, frame count 1460000, epsilon 0.154, loss 0.00049\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./breakout_alpha_progress_episodes_10000.44/assets\n",
      "running reward: 1.12 at episode 10041, frame count 1470000, epsilon 0.153, loss 0.00122\n",
      "running reward: 1.06 at episode 10087, frame count 1480000, epsilon 0.152, loss 0.00056\n",
      "running reward: 1.10 at episode 10124, frame count 1490000, epsilon 0.151, loss 0.00103\n",
      "running reward: 1.19 at episode 10162, frame count 1500000, epsilon 0.150, loss 0.00220\n",
      "running reward: 1.23 at episode 10204, frame count 1510000, epsilon 0.149, loss 0.00338\n",
      "running reward: 1.17 at episode 10246, frame count 1520000, epsilon 0.148, loss 0.00101\n",
      "running reward: 1.15 at episode 10286, frame count 1530000, epsilon 0.147, loss 0.00038\n",
      "running reward: 1.16 at episode 10326, frame count 1540000, epsilon 0.146, loss 0.00177\n",
      "running reward: 1.19 at episode 10364, frame count 1550000, epsilon 0.145, loss 0.00320\n",
      "running reward: 1.19 at episode 10404, frame count 1560000, epsilon 0.144, loss 0.00071\n",
      "running reward: 1.24 at episode 10443, frame count 1570000, epsilon 0.143, loss 0.00037\n",
      "running reward: 1.19 at episode 10481, frame count 1580000, epsilon 0.142, loss 0.00112\n",
      "running reward: 1.19 at episode 10519, frame count 1590000, epsilon 0.141, loss 0.00037\n",
      "running reward: 1.22 at episode 10555, frame count 1600000, epsilon 0.140, loss 0.00040\n",
      "running reward: 1.13 at episode 10595, frame count 1610000, epsilon 0.139, loss 0.00034\n",
      "running reward: 1.02 at episode 10637, frame count 1620000, epsilon 0.138, loss 0.00111\n",
      "running reward: 1.11 at episode 10669, frame count 1630000, epsilon 0.137, loss 0.00103\n",
      "running reward: 1.16 at episode 10709, frame count 1640000, epsilon 0.136, loss 0.00044\n",
      "running reward: 1.20 at episode 10747, frame count 1650000, epsilon 0.135, loss 0.00281\n",
      "running reward: 1.10 at episode 10787, frame count 1660000, epsilon 0.134, loss 0.00095\n",
      "running reward: 1.12 at episode 10827, frame count 1670000, epsilon 0.133, loss 0.00052\n",
      "running reward: 1.05 at episode 10867, frame count 1680000, epsilon 0.132, loss 0.00064\n",
      "running reward: 0.97 at episode 10911, frame count 1690000, epsilon 0.131, loss 0.00329\n",
      "running reward: 0.93 at episode 10952, frame count 1700000, epsilon 0.130, loss 0.00102\n",
      "running reward: 1.00 at episode 10990, frame count 1710000, epsilon 0.129, loss 0.00038\n",
      "running reward: 1.14 at episode 11026, frame count 1720000, epsilon 0.128, loss 0.00463\n",
      "running reward: 1.23 at episode 11063, frame count 1730000, epsilon 0.127, loss 0.01139\n",
      "running reward: 1.26 at episode 11102, frame count 1740000, epsilon 0.126, loss 0.00126\n",
      "running reward: 1.19 at episode 11142, frame count 1750000, epsilon 0.125, loss 0.00024\n",
      "running reward: 1.32 at episode 11174, frame count 1760000, epsilon 0.124, loss 0.00060\n",
      "running reward: 1.32 at episode 11214, frame count 1770000, epsilon 0.123, loss 0.00360\n",
      "running reward: 1.29 at episode 11253, frame count 1780000, epsilon 0.122, loss 0.00091\n",
      "running reward: 1.26 at episode 11291, frame count 1790000, epsilon 0.121, loss 0.00060\n",
      "running reward: 1.14 at episode 11335, frame count 1800000, epsilon 0.120, loss 0.00045\n",
      "running reward: 1.02 at episode 11383, frame count 1810000, epsilon 0.119, loss 0.00806\n",
      "running reward: 1.03 at episode 11424, frame count 1820000, epsilon 0.118, loss 0.00036\n",
      "running reward: 1.17 at episode 11462, frame count 1830000, epsilon 0.117, loss 0.00085\n",
      "running reward: 1.21 at episode 11503, frame count 1840000, epsilon 0.116, loss 0.00136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 1.36 at episode 11535, frame count 1850000, epsilon 0.115, loss 0.00074\n",
      "running reward: 1.38 at episode 11571, frame count 1860000, epsilon 0.114, loss 0.00712\n",
      "running reward: 1.41 at episode 11612, frame count 1870000, epsilon 0.113, loss 0.00031\n",
      "running reward: 1.16 at episode 11655, frame count 1880000, epsilon 0.112, loss 0.00072\n",
      "running reward: 1.24 at episode 11687, frame count 1890000, epsilon 0.111, loss 0.00172\n",
      "running reward: 1.33 at episode 11723, frame count 1900000, epsilon 0.110, loss 0.00071\n",
      "running reward: 1.34 at episode 11760, frame count 1910000, epsilon 0.109, loss 0.00173\n",
      "running reward: 1.27 at episode 11796, frame count 1920000, epsilon 0.108, loss 0.00966\n",
      "running reward: 1.34 at episode 11832, frame count 1930000, epsilon 0.107, loss 0.00466\n",
      "running reward: 1.23 at episode 11874, frame count 1940000, epsilon 0.106, loss 0.00074\n",
      "running reward: 1.14 at episode 11913, frame count 1950000, epsilon 0.105, loss 0.00092\n",
      "running reward: 1.10 at episode 11954, frame count 1960000, epsilon 0.104, loss 0.00088\n",
      "running reward: 1.25 at episode 11985, frame count 1970000, epsilon 0.103, loss 0.00046\n",
      "running reward: 1.40 at episode 12016, frame count 1980000, epsilon 0.102, loss 0.00082\n",
      "running reward: 1.53 at episode 12050, frame count 1990000, epsilon 0.101, loss 0.00137\n",
      "running reward: 1.50 at episode 12085, frame count 2000000, epsilon 0.100, loss 0.00030\n",
      "running reward: 1.44 at episode 12119, frame count 2010000, epsilon 0.099, loss 0.00219\n",
      "running reward: 1.32 at episode 12158, frame count 2020000, epsilon 0.098, loss 0.00951\n",
      "running reward: 1.46 at episode 12186, frame count 2030000, epsilon 0.098, loss 0.00050\n",
      "running reward: 1.54 at episode 12215, frame count 2040000, epsilon 0.097, loss 0.00245\n",
      "running reward: 1.56 at episode 12251, frame count 2050000, epsilon 0.096, loss 0.00059\n",
      "running reward: 1.55 at episode 12281, frame count 2060000, epsilon 0.095, loss 0.00057\n",
      "running reward: 1.41 at episode 12315, frame count 2070000, epsilon 0.094, loss 0.00277\n",
      "running reward: 1.46 at episode 12349, frame count 2080000, epsilon 0.094, loss 0.00871\n",
      "running reward: 1.60 at episode 12376, frame count 2090000, epsilon 0.093, loss 0.00100\n",
      "running reward: 1.59 at episode 12408, frame count 2100000, epsilon 0.092, loss 0.00097\n",
      "running reward: 1.70 at episode 12437, frame count 2110000, epsilon 0.091, loss 0.00935\n",
      "running reward: 1.80 at episode 12464, frame count 2120000, epsilon 0.090, loss 0.00117\n",
      "running reward: 1.84 at episode 12489, frame count 2130000, epsilon 0.090, loss 0.00172\n",
      "running reward: 1.77 at episode 12518, frame count 2140000, epsilon 0.089, loss 0.00121\n",
      "running reward: 1.83 at episode 12546, frame count 2150000, epsilon 0.088, loss 0.00060\n",
      "running reward: 1.82 at episode 12572, frame count 2160000, epsilon 0.087, loss 0.00081\n",
      "running reward: 1.61 at episode 12609, frame count 2170000, epsilon 0.086, loss 0.00266\n",
      "running reward: 1.58 at episode 12638, frame count 2180000, epsilon 0.086, loss 0.00056\n",
      "running reward: 1.57 at episode 12666, frame count 2190000, epsilon 0.085, loss 0.00064\n",
      "running reward: 1.62 at episode 12695, frame count 2200000, epsilon 0.084, loss 0.00086\n",
      "running reward: 1.57 at episode 12734, frame count 2210000, epsilon 0.083, loss 0.00844\n",
      "running reward: 1.61 at episode 12760, frame count 2220000, epsilon 0.082, loss 0.00203\n",
      "running reward: 1.39 at episode 12795, frame count 2230000, epsilon 0.082, loss 0.00065\n",
      "running reward: 1.64 at episode 12822, frame count 2240000, epsilon 0.081, loss 0.00137\n",
      "running reward: 1.70 at episode 12846, frame count 2250000, epsilon 0.080, loss 0.00355\n",
      "running reward: 1.87 at episode 12873, frame count 2260000, epsilon 0.079, loss 0.00125\n",
      "running reward: 2.10 at episode 12899, frame count 2270000, epsilon 0.078, loss 0.00106\n",
      "running reward: 2.06 at episode 12929, frame count 2280000, epsilon 0.078, loss 0.00243\n",
      "running reward: 2.01 at episode 12953, frame count 2290000, epsilon 0.077, loss 0.00262\n",
      "running reward: 1.93 at episode 12981, frame count 2300000, epsilon 0.076, loss 0.00041\n",
      "running reward: 2.03 at episode 13005, frame count 2310000, epsilon 0.075, loss 0.00081\n",
      "running reward: 2.11 at episode 13030, frame count 2320000, epsilon 0.074, loss 0.00108\n",
      "running reward: 2.08 at episode 13058, frame count 2330000, epsilon 0.074, loss 0.00309\n",
      "running reward: 2.32 at episode 13076, frame count 2340000, epsilon 0.073, loss 0.00056\n",
      "running reward: 2.47 at episode 13095, frame count 2350000, epsilon 0.072, loss 0.00768\n",
      "running reward: 2.58 at episode 13119, frame count 2360000, epsilon 0.071, loss 0.00189\n",
      "running reward: 2.58 at episode 13141, frame count 2370000, epsilon 0.070, loss 0.00334\n",
      "running reward: 2.82 at episode 13160, frame count 2380000, epsilon 0.070, loss 0.00170\n",
      "running reward: 2.63 at episode 13183, frame count 2390000, epsilon 0.069, loss 0.00125\n",
      "running reward: 2.72 at episode 13204, frame count 2400000, epsilon 0.068, loss 0.00081\n",
      "running reward: 2.41 at episode 13234, frame count 2410000, epsilon 0.067, loss 0.00068\n",
      "running reward: 2.54 at episode 13251, frame count 2420000, epsilon 0.066, loss 0.00095\n",
      "running reward: 2.67 at episode 13270, frame count 2430000, epsilon 0.066, loss 0.00048\n",
      "running reward: 2.66 at episode 13294, frame count 2440000, epsilon 0.065, loss 0.00144\n",
      "running reward: 2.63 at episode 13315, frame count 2450000, epsilon 0.064, loss 0.00106\n",
      "running reward: 3.04 at episode 13331, frame count 2460000, epsilon 0.063, loss 0.00127\n",
      "running reward: 3.01 at episode 13351, frame count 2470000, epsilon 0.062, loss 0.00054\n",
      "running reward: 3.16 at episode 13366, frame count 2480000, epsilon 0.062, loss 0.00386\n",
      "running reward: 3.13 at episode 13388, frame count 2490000, epsilon 0.061, loss 0.00094\n",
      "running reward: 3.06 at episode 13413, frame count 2500000, epsilon 0.060, loss 0.00090\n",
      "running reward: 2.54 at episode 13441, frame count 2510000, epsilon 0.059, loss 0.00073\n",
      "running reward: 2.43 at episode 13462, frame count 2520000, epsilon 0.058, loss 0.00071\n",
      "running reward: 2.39 at episode 13485, frame count 2530000, epsilon 0.058, loss 0.00129\n",
      "running reward: 2.41 at episode 13509, frame count 2540000, epsilon 0.057, loss 0.00515\n",
      "running reward: 2.58 at episode 13530, frame count 2550000, epsilon 0.056, loss 0.00220\n",
      "running reward: 2.63 at episode 13554, frame count 2560000, epsilon 0.055, loss 0.00260\n",
      "running reward: 2.75 at episode 13573, frame count 2570000, epsilon 0.054, loss 0.00060\n",
      "running reward: 2.66 at episode 13597, frame count 2580000, epsilon 0.054, loss 0.00064\n",
      "running reward: 2.54 at episode 13622, frame count 2590000, epsilon 0.053, loss 0.00055\n",
      "running reward: 2.56 at episode 13646, frame count 2600000, epsilon 0.052, loss 0.00050\n",
      "running reward: 1.98 at episode 13678, frame count 2610000, epsilon 0.051, loss 0.00428\n",
      "running reward: 2.24 at episode 13698, frame count 2620000, epsilon 0.050, loss 0.00142\n",
      "running reward: 2.37 at episode 13718, frame count 2630000, epsilon 0.050, loss 0.00067\n",
      "running reward: 2.60 at episode 13735, frame count 2640000, epsilon 0.049, loss 0.00180\n",
      "running reward: 2.93 at episode 13751, frame count 2650000, epsilon 0.048, loss 0.00356\n",
      "running reward: 3.32 at episode 13766, frame count 2660000, epsilon 0.047, loss 0.00201\n",
      "running reward: 3.53 at episode 13783, frame count 2670000, epsilon 0.046, loss 0.00564\n",
      "running reward: 3.60 at episode 13802, frame count 2680000, epsilon 0.046, loss 0.00070\n",
      "running reward: 3.37 at episode 13822, frame count 2690000, epsilon 0.045, loss 0.00053\n",
      "running reward: 3.19 at episode 13845, frame count 2700000, epsilon 0.044, loss 0.00169\n",
      "running reward: 3.17 at episode 13863, frame count 2710000, epsilon 0.043, loss 0.00132\n",
      "running reward: 3.08 at episode 13880, frame count 2720000, epsilon 0.042, loss 0.00137\n",
      "running reward: 2.97 at episode 13900, frame count 2730000, epsilon 0.042, loss 0.00119\n",
      "running reward: 3.21 at episode 13916, frame count 2740000, epsilon 0.041, loss 0.00178\n",
      "running reward: 3.39 at episode 13934, frame count 2750000, epsilon 0.040, loss 0.00166\n",
      "running reward: 3.38 at episode 13953, frame count 2760000, epsilon 0.039, loss 0.00166\n",
      "running reward: 3.47 at episode 13970, frame count 2770000, epsilon 0.038, loss 0.00188\n",
      "running reward: 3.55 at episode 13986, frame count 2780000, epsilon 0.038, loss 0.00144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 3.62 at episode 14005, frame count 2790000, epsilon 0.037, loss 0.00084\n",
      "running reward: 3.54 at episode 14023, frame count 2800000, epsilon 0.036, loss 0.00058\n",
      "running reward: 3.54 at episode 14039, frame count 2810000, epsilon 0.035, loss 0.00779\n",
      "running reward: 3.70 at episode 14056, frame count 2820000, epsilon 0.034, loss 0.00148\n",
      "running reward: 3.78 at episode 14070, frame count 2830000, epsilon 0.034, loss 0.01309\n",
      "running reward: 3.94 at episode 14085, frame count 2840000, epsilon 0.033, loss 0.00114\n",
      "running reward: 3.96 at episode 14101, frame count 2850000, epsilon 0.032, loss 0.00127\n",
      "running reward: 3.83 at episode 14121, frame count 2860000, epsilon 0.031, loss 0.00100\n",
      "running reward: 3.86 at episode 14137, frame count 2870000, epsilon 0.030, loss 0.00075\n",
      "running reward: 3.92 at episode 14155, frame count 2880000, epsilon 0.030, loss 0.00167\n",
      "running reward: 3.92 at episode 14167, frame count 2890000, epsilon 0.029, loss 0.00202\n",
      "running reward: 3.73 at episode 14185, frame count 2900000, epsilon 0.028, loss 0.00213\n",
      "running reward: 3.85 at episode 14200, frame count 2910000, epsilon 0.027, loss 0.00058\n",
      "running reward: 4.00 at episode 14215, frame count 2920000, epsilon 0.026, loss 0.00079\n",
      "running reward: 4.30 at episode 14228, frame count 2930000, epsilon 0.026, loss 0.00033\n",
      "running reward: 4.00 at episode 14251, frame count 2940000, epsilon 0.025, loss 0.00164\n",
      "running reward: 3.93 at episode 14265, frame count 2950000, epsilon 0.024, loss 0.00060\n",
      "running reward: 4.18 at episode 14278, frame count 2960000, epsilon 0.023, loss 0.00122\n",
      "running reward: 3.97 at episode 14297, frame count 2970000, epsilon 0.022, loss 0.00051\n",
      "running reward: 3.80 at episode 14312, frame count 2980000, epsilon 0.022, loss 0.00086\n",
      "running reward: 3.76 at episode 14328, frame count 2990000, epsilon 0.021, loss 0.00141\n",
      "running reward: 4.02 at episode 14342, frame count 3000000, epsilon 0.020, loss 0.00101\n",
      "running reward: 4.15 at episode 14355, frame count 3010000, epsilon 0.020, loss 0.00232\n",
      "running reward: 4.05 at episode 14373, frame count 3020000, epsilon 0.020, loss 0.00109\n",
      "running reward: 4.27 at episode 14386, frame count 3030000, epsilon 0.020, loss 0.00061\n",
      "running reward: 4.14 at episode 14403, frame count 3040000, epsilon 0.020, loss 0.00069\n",
      "running reward: 3.86 at episode 14422, frame count 3050000, epsilon 0.020, loss 0.00105\n",
      "running reward: 4.21 at episode 14436, frame count 3060000, epsilon 0.020, loss 0.00506\n",
      "running reward: 3.89 at episode 14455, frame count 3070000, epsilon 0.020, loss 0.00182\n",
      "running reward: 3.67 at episode 14476, frame count 3080000, epsilon 0.020, loss 0.00110\n",
      "running reward: 3.47 at episode 14492, frame count 3090000, epsilon 0.020, loss 0.00064\n",
      "running reward: 3.43 at episode 14512, frame count 3100000, epsilon 0.020, loss 0.00197\n",
      "running reward: 3.62 at episode 14526, frame count 3110000, epsilon 0.020, loss 0.00148\n",
      "running reward: 3.70 at episode 14535, frame count 3120000, epsilon 0.020, loss 0.00068\n",
      "running reward: 4.01 at episode 14548, frame count 3130000, epsilon 0.020, loss 0.00039\n",
      "running reward: 4.12 at episode 14565, frame count 3140000, epsilon 0.020, loss 0.01841\n",
      "running reward: 4.34 at episode 14580, frame count 3150000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.34 at episode 14598, frame count 3160000, epsilon 0.020, loss 0.00156\n",
      "running reward: 4.19 at episode 14617, frame count 3170000, epsilon 0.020, loss 0.00066\n",
      "running reward: 3.66 at episode 14636, frame count 3180000, epsilon 0.020, loss 0.00159\n",
      "running reward: 3.28 at episode 14658, frame count 3190000, epsilon 0.020, loss 0.00121\n",
      "running reward: 2.98 at episode 14681, frame count 3200000, epsilon 0.020, loss 0.00514\n",
      "running reward: 2.83 at episode 14700, frame count 3210000, epsilon 0.020, loss 0.00029\n",
      "running reward: 2.82 at episode 14720, frame count 3220000, epsilon 0.020, loss 0.00051\n",
      "running reward: 3.02 at episode 14736, frame count 3230000, epsilon 0.020, loss 0.00052\n",
      "running reward: 3.10 at episode 14754, frame count 3240000, epsilon 0.020, loss 0.00218\n",
      "running reward: 3.18 at episode 14767, frame count 3250000, epsilon 0.020, loss 0.00064\n",
      "running reward: 3.48 at episode 14787, frame count 3260000, epsilon 0.020, loss 0.00062\n",
      "running reward: 3.52 at episode 14810, frame count 3270000, epsilon 0.020, loss 0.00041\n",
      "running reward: 3.31 at episode 14830, frame count 3280000, epsilon 0.020, loss 0.00114\n",
      "running reward: 3.21 at episode 14849, frame count 3290000, epsilon 0.020, loss 0.00046\n",
      "running reward: 3.07 at episode 14867, frame count 3300000, epsilon 0.020, loss 0.00102\n",
      "running reward: 2.99 at episode 14887, frame count 3310000, epsilon 0.020, loss 0.00089\n",
      "running reward: 3.11 at episode 14906, frame count 3320000, epsilon 0.020, loss 0.01794\n",
      "running reward: 3.24 at episode 14926, frame count 3330000, epsilon 0.020, loss 0.00060\n",
      "running reward: 3.09 at episode 14942, frame count 3340000, epsilon 0.020, loss 0.00096\n",
      "running reward: 3.61 at episode 14953, frame count 3350000, epsilon 0.020, loss 0.00057\n",
      "running reward: 3.64 at episode 14967, frame count 3360000, epsilon 0.020, loss 0.00103\n",
      "running reward: 3.91 at episode 14982, frame count 3370000, epsilon 0.020, loss 0.00029\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./breakout_alpha_progress_episodes_15000.44/assets\n",
      "running reward: 3.71 at episode 15000, frame count 3380000, epsilon 0.020, loss 0.00114\n",
      "running reward: 4.00 at episode 15015, frame count 3390000, epsilon 0.020, loss 0.00061\n",
      "running reward: 4.18 at episode 15030, frame count 3400000, epsilon 0.020, loss 0.00244\n",
      "running reward: 4.23 at episode 15044, frame count 3410000, epsilon 0.020, loss 0.00353\n",
      "running reward: 3.77 at episode 15065, frame count 3420000, epsilon 0.020, loss 0.00044\n",
      "running reward: 3.73 at episode 15081, frame count 3430000, epsilon 0.020, loss 0.00038\n",
      "running reward: 3.90 at episode 15095, frame count 3440000, epsilon 0.020, loss 0.00058\n",
      "running reward: 3.76 at episode 15113, frame count 3450000, epsilon 0.020, loss 0.00323\n",
      "running reward: 3.33 at episode 15138, frame count 3460000, epsilon 0.020, loss 0.00037\n",
      "running reward: 3.22 at episode 15155, frame count 3470000, epsilon 0.020, loss 0.00039\n",
      "running reward: 3.52 at episode 15171, frame count 3480000, epsilon 0.020, loss 0.00128\n",
      "running reward: 3.23 at episode 15189, frame count 3490000, epsilon 0.020, loss 0.00127\n",
      "running reward: 3.34 at episode 15203, frame count 3500000, epsilon 0.020, loss 0.00054\n",
      "running reward: 3.57 at episode 15215, frame count 3510000, epsilon 0.020, loss 0.00065\n",
      "running reward: 3.87 at episode 15233, frame count 3520000, epsilon 0.020, loss 0.00074\n",
      "running reward: 3.98 at episode 15247, frame count 3530000, epsilon 0.020, loss 0.00170\n",
      "running reward: 4.10 at episode 15262, frame count 3540000, epsilon 0.020, loss 0.00077\n",
      "running reward: 4.14 at episode 15278, frame count 3550000, epsilon 0.020, loss 0.00182\n",
      "running reward: 4.30 at episode 15292, frame count 3560000, epsilon 0.020, loss 0.00062\n",
      "running reward: 4.18 at episode 15306, frame count 3570000, epsilon 0.020, loss 0.00060\n",
      "running reward: 3.89 at episode 15327, frame count 3580000, epsilon 0.020, loss 0.00023\n",
      "running reward: 4.04 at episode 15342, frame count 3590000, epsilon 0.020, loss 0.00040\n",
      "running reward: 4.16 at episode 15352, frame count 3600000, epsilon 0.020, loss 0.00162\n",
      "running reward: 4.26 at episode 15366, frame count 3610000, epsilon 0.020, loss 0.00038\n",
      "running reward: 4.27 at episode 15382, frame count 3620000, epsilon 0.020, loss 0.00282\n",
      "running reward: 4.42 at episode 15391, frame count 3630000, epsilon 0.020, loss 0.00460\n",
      "running reward: 4.38 at episode 15409, frame count 3640000, epsilon 0.020, loss 0.00086\n",
      "running reward: 4.49 at episode 15426, frame count 3650000, epsilon 0.020, loss 0.00051\n",
      "running reward: 3.92 at episode 15449, frame count 3660000, epsilon 0.020, loss 0.00074\n",
      "running reward: 3.69 at episode 15466, frame count 3670000, epsilon 0.020, loss 0.00235\n",
      "running reward: 3.76 at episode 15481, frame count 3680000, epsilon 0.020, loss 0.00072\n",
      "running reward: 3.49 at episode 15495, frame count 3690000, epsilon 0.020, loss 0.00048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 3.49 at episode 15512, frame count 3700000, epsilon 0.020, loss 0.00052\n",
      "running reward: 3.41 at episode 15531, frame count 3710000, epsilon 0.020, loss 0.00405\n",
      "running reward: 3.80 at episode 15543, frame count 3720000, epsilon 0.020, loss 0.00031\n",
      "running reward: 3.69 at episode 15564, frame count 3730000, epsilon 0.020, loss 0.01399\n",
      "running reward: 3.44 at episode 15588, frame count 3740000, epsilon 0.020, loss 0.00206\n",
      "running reward: 3.47 at episode 15599, frame count 3750000, epsilon 0.020, loss 0.00082\n",
      "running reward: 3.52 at episode 15616, frame count 3760000, epsilon 0.020, loss 0.00043\n",
      "running reward: 3.72 at episode 15631, frame count 3770000, epsilon 0.020, loss 0.00105\n",
      "running reward: 3.50 at episode 15646, frame count 3780000, epsilon 0.020, loss 0.00039\n",
      "running reward: 3.77 at episode 15662, frame count 3790000, epsilon 0.020, loss 0.00103\n",
      "running reward: 4.06 at episode 15676, frame count 3800000, epsilon 0.020, loss 0.00186\n",
      "running reward: 4.17 at episode 15693, frame count 3810000, epsilon 0.020, loss 0.00076\n",
      "running reward: 4.07 at episode 15705, frame count 3820000, epsilon 0.020, loss 0.00089\n",
      "running reward: 3.92 at episode 15726, frame count 3830000, epsilon 0.020, loss 0.00068\n",
      "running reward: 3.58 at episode 15747, frame count 3840000, epsilon 0.020, loss 0.00085\n",
      "running reward: 3.51 at episode 15763, frame count 3850000, epsilon 0.020, loss 0.00266\n",
      "running reward: 3.80 at episode 15775, frame count 3860000, epsilon 0.020, loss 0.00662\n",
      "running reward: 3.86 at episode 15790, frame count 3870000, epsilon 0.020, loss 0.01335\n",
      "running reward: 3.85 at episode 15805, frame count 3880000, epsilon 0.020, loss 0.00059\n",
      "running reward: 3.92 at episode 15820, frame count 3890000, epsilon 0.020, loss 0.00103\n",
      "running reward: 4.21 at episode 15836, frame count 3900000, epsilon 0.020, loss 0.00068\n",
      "running reward: 4.46 at episode 15846, frame count 3910000, epsilon 0.020, loss 0.00462\n",
      "running reward: 4.65 at episode 15860, frame count 3920000, epsilon 0.020, loss 0.00620\n",
      "running reward: 4.38 at episode 15875, frame count 3930000, epsilon 0.020, loss 0.00236\n",
      "running reward: 4.37 at episode 15890, frame count 3940000, epsilon 0.020, loss 0.00091\n",
      "running reward: 4.33 at episode 15906, frame count 3950000, epsilon 0.020, loss 0.00044\n",
      "running reward: 4.43 at episode 15920, frame count 3960000, epsilon 0.020, loss 0.00220\n",
      "running reward: 4.52 at episode 15935, frame count 3970000, epsilon 0.020, loss 0.00072\n",
      "running reward: 4.35 at episode 15947, frame count 3980000, epsilon 0.020, loss 0.00021\n",
      "running reward: 4.25 at episode 15962, frame count 3990000, epsilon 0.020, loss 0.00066\n",
      "running reward: 4.50 at episode 15975, frame count 4000000, epsilon 0.020, loss 0.00048\n",
      "running reward: 4.39 at episode 15991, frame count 4010000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.00 at episode 16014, frame count 4020000, epsilon 0.020, loss 0.00034\n",
      "running reward: 3.67 at episode 16036, frame count 4030000, epsilon 0.020, loss 0.00337\n",
      "running reward: 3.35 at episode 16055, frame count 4040000, epsilon 0.020, loss 0.00087\n",
      "running reward: 3.32 at episode 16069, frame count 4050000, epsilon 0.020, loss 0.00348\n",
      "running reward: 3.27 at episode 16083, frame count 4060000, epsilon 0.020, loss 0.01863\n",
      "running reward: 3.50 at episode 16096, frame count 4070000, epsilon 0.020, loss 0.00035\n",
      "running reward: 3.72 at episode 16107, frame count 4080000, epsilon 0.020, loss 0.00118\n",
      "running reward: 4.29 at episode 16121, frame count 4090000, epsilon 0.020, loss 0.00940\n",
      "running reward: 4.55 at episode 16134, frame count 4100000, epsilon 0.020, loss 0.00082\n",
      "running reward: 4.76 at episode 16145, frame count 4110000, epsilon 0.020, loss 0.00058\n",
      "running reward: 5.11 at episode 16157, frame count 4120000, epsilon 0.020, loss 0.00085\n",
      "running reward: 4.91 at episode 16173, frame count 4130000, epsilon 0.020, loss 0.00057\n",
      "running reward: 4.85 at episode 16188, frame count 4140000, epsilon 0.020, loss 0.00058\n",
      "running reward: 4.74 at episode 16204, frame count 4150000, epsilon 0.020, loss 0.00587\n",
      "running reward: 4.53 at episode 16218, frame count 4160000, epsilon 0.020, loss 0.00172\n",
      "running reward: 4.47 at episode 16231, frame count 4170000, epsilon 0.020, loss 0.00047\n",
      "running reward: 4.28 at episode 16246, frame count 4180000, epsilon 0.020, loss 0.00051\n",
      "running reward: 4.22 at episode 16259, frame count 4190000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.23 at episode 16274, frame count 4200000, epsilon 0.020, loss 0.00112\n",
      "running reward: 4.42 at episode 16287, frame count 4210000, epsilon 0.020, loss 0.00029\n",
      "running reward: 4.37 at episode 16303, frame count 4220000, epsilon 0.020, loss 0.00035\n",
      "running reward: 4.34 at episode 16319, frame count 4230000, epsilon 0.020, loss 0.00081\n",
      "running reward: 4.29 at episode 16335, frame count 4240000, epsilon 0.020, loss 0.00112\n",
      "running reward: 3.99 at episode 16353, frame count 4250000, epsilon 0.020, loss 0.00027\n",
      "running reward: 3.83 at episode 16367, frame count 4260000, epsilon 0.020, loss 0.00109\n",
      "running reward: 4.24 at episode 16378, frame count 4270000, epsilon 0.020, loss 0.00035\n",
      "running reward: 3.92 at episode 16395, frame count 4280000, epsilon 0.020, loss 0.00059\n",
      "running reward: 4.08 at episode 16410, frame count 4290000, epsilon 0.020, loss 0.00115\n",
      "running reward: 3.78 at episode 16429, frame count 4300000, epsilon 0.020, loss 0.00023\n",
      "running reward: 3.73 at episode 16449, frame count 4310000, epsilon 0.020, loss 0.00122\n",
      "running reward: 3.78 at episode 16466, frame count 4320000, epsilon 0.020, loss 0.00189\n",
      "running reward: 3.39 at episode 16486, frame count 4330000, epsilon 0.020, loss 0.00218\n",
      "running reward: 3.29 at episode 16503, frame count 4340000, epsilon 0.020, loss 0.00101\n",
      "running reward: 3.41 at episode 16518, frame count 4350000, epsilon 0.020, loss 0.00074\n",
      "running reward: 3.52 at episode 16533, frame count 4360000, epsilon 0.020, loss 0.00046\n",
      "running reward: 3.54 at episode 16552, frame count 4370000, epsilon 0.020, loss 0.00302\n",
      "running reward: 3.66 at episode 16566, frame count 4380000, epsilon 0.020, loss 0.00060\n",
      "running reward: 3.69 at episode 16582, frame count 4390000, epsilon 0.020, loss 0.00054\n",
      "running reward: 3.79 at episode 16599, frame count 4400000, epsilon 0.020, loss 0.00057\n",
      "running reward: 4.07 at episode 16610, frame count 4410000, epsilon 0.020, loss 0.00030\n",
      "running reward: 4.21 at episode 16622, frame count 4420000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.19 at episode 16636, frame count 4430000, epsilon 0.020, loss 0.00027\n",
      "running reward: 4.57 at episode 16650, frame count 4440000, epsilon 0.020, loss 0.00049\n",
      "running reward: 4.49 at episode 16665, frame count 4450000, epsilon 0.020, loss 0.00086\n",
      "running reward: 4.71 at episode 16677, frame count 4460000, epsilon 0.020, loss 0.00074\n",
      "running reward: 4.79 at episode 16689, frame count 4470000, epsilon 0.020, loss 0.00064\n",
      "running reward: 5.00 at episode 16701, frame count 4480000, epsilon 0.020, loss 0.00023\n",
      "running reward: 4.87 at episode 16715, frame count 4490000, epsilon 0.020, loss 0.00387\n",
      "running reward: 4.85 at episode 16726, frame count 4500000, epsilon 0.020, loss 0.00081\n",
      "running reward: 5.09 at episode 16736, frame count 4510000, epsilon 0.020, loss 0.00168\n",
      "running reward: 5.24 at episode 16748, frame count 4520000, epsilon 0.020, loss 0.00047\n",
      "running reward: 5.50 at episode 16760, frame count 4530000, epsilon 0.020, loss 0.02044\n",
      "running reward: 5.51 at episode 16770, frame count 4540000, epsilon 0.020, loss 0.00480\n",
      "running reward: 5.72 at episode 16780, frame count 4550000, epsilon 0.020, loss 0.00243\n",
      "running reward: 5.69 at episode 16792, frame count 4560000, epsilon 0.020, loss 0.00348\n",
      "running reward: 5.71 at episode 16804, frame count 4570000, epsilon 0.020, loss 0.00059\n",
      "running reward: 5.44 at episode 16821, frame count 4580000, epsilon 0.020, loss 0.00616\n",
      "running reward: 5.14 at episode 16837, frame count 4590000, epsilon 0.020, loss 0.00107\n",
      "running reward: 5.03 at episode 16849, frame count 4600000, epsilon 0.020, loss 0.00117\n",
      "running reward: 4.62 at episode 16867, frame count 4610000, epsilon 0.020, loss 0.00344\n",
      "running reward: 4.18 at episode 16884, frame count 4620000, epsilon 0.020, loss 0.00266\n",
      "running reward: 3.96 at episode 16901, frame count 4630000, epsilon 0.020, loss 0.00145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 3.93 at episode 16915, frame count 4640000, epsilon 0.020, loss 0.00064\n",
      "running reward: 4.07 at episode 16928, frame count 4650000, epsilon 0.020, loss 0.00041\n",
      "running reward: 3.83 at episode 16944, frame count 4660000, epsilon 0.020, loss 0.00035\n",
      "running reward: 4.20 at episode 16954, frame count 4670000, epsilon 0.020, loss 0.00312\n",
      "running reward: 4.20 at episode 16970, frame count 4680000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.31 at episode 16984, frame count 4690000, epsilon 0.020, loss 0.00083\n",
      "running reward: 4.45 at episode 16998, frame count 4700000, epsilon 0.020, loss 0.00041\n",
      "running reward: 4.43 at episode 17015, frame count 4710000, epsilon 0.020, loss 0.00281\n",
      "running reward: 4.34 at episode 17027, frame count 4720000, epsilon 0.020, loss 0.00039\n",
      "running reward: 4.38 at episode 17043, frame count 4730000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.29 at episode 17054, frame count 4740000, epsilon 0.020, loss 0.00087\n",
      "running reward: 4.52 at episode 17066, frame count 4750000, epsilon 0.020, loss 0.00820\n",
      "running reward: 4.61 at episode 17079, frame count 4760000, epsilon 0.020, loss 0.00450\n",
      "running reward: 4.53 at episode 17097, frame count 4770000, epsilon 0.020, loss 0.00136\n",
      "running reward: 4.69 at episode 17109, frame count 4780000, epsilon 0.020, loss 0.00172\n",
      "running reward: 4.79 at episode 17120, frame count 4790000, epsilon 0.020, loss 0.01777\n",
      "running reward: 4.70 at episode 17134, frame count 4800000, epsilon 0.020, loss 0.00066\n",
      "running reward: 4.49 at episode 17154, frame count 4810000, epsilon 0.020, loss 0.00129\n",
      "running reward: 4.05 at episode 17171, frame count 4820000, epsilon 0.020, loss 0.00114\n",
      "running reward: 4.09 at episode 17185, frame count 4830000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.32 at episode 17197, frame count 4840000, epsilon 0.020, loss 0.00043\n",
      "running reward: 4.28 at episode 17210, frame count 4850000, epsilon 0.020, loss 0.01333\n",
      "running reward: 4.03 at episode 17226, frame count 4860000, epsilon 0.020, loss 0.00283\n",
      "running reward: 3.98 at episode 17242, frame count 4870000, epsilon 0.020, loss 0.00113\n",
      "running reward: 4.21 at episode 17255, frame count 4880000, epsilon 0.020, loss 0.02847\n",
      "running reward: 4.35 at episode 17270, frame count 4890000, epsilon 0.020, loss 0.00038\n",
      "running reward: 4.30 at episode 17287, frame count 4900000, epsilon 0.020, loss 0.00071\n",
      "running reward: 4.19 at episode 17299, frame count 4910000, epsilon 0.020, loss 0.00074\n",
      "running reward: 4.23 at episode 17316, frame count 4920000, epsilon 0.020, loss 0.00117\n",
      "running reward: 4.21 at episode 17333, frame count 4930000, epsilon 0.020, loss 0.00074\n",
      "running reward: 4.11 at episode 17350, frame count 4940000, epsilon 0.020, loss 0.00055\n",
      "running reward: 4.11 at episode 17366, frame count 4950000, epsilon 0.020, loss 0.00046\n",
      "running reward: 4.35 at episode 17376, frame count 4960000, epsilon 0.020, loss 0.00175\n",
      "running reward: 4.58 at episode 17386, frame count 4970000, epsilon 0.020, loss 0.00089\n",
      "running reward: 4.69 at episode 17395, frame count 4980000, epsilon 0.020, loss 0.00077\n",
      "running reward: 4.87 at episode 17409, frame count 4990000, epsilon 0.020, loss 0.00101\n",
      "running reward: 5.13 at episode 17419, frame count 5000000, epsilon 0.020, loss 0.00047\n",
      "running reward: 5.18 at episode 17431, frame count 5010000, epsilon 0.020, loss 0.00045\n",
      "running reward: 5.26 at episode 17449, frame count 5020000, epsilon 0.020, loss 0.00582\n",
      "running reward: 5.44 at episode 17460, frame count 5030000, epsilon 0.020, loss 0.00097\n",
      "running reward: 5.30 at episode 17472, frame count 5040000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.19 at episode 17484, frame count 5050000, epsilon 0.020, loss 0.00052\n",
      "running reward: 5.28 at episode 17495, frame count 5060000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.04 at episode 17511, frame count 5070000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.81 at episode 17526, frame count 5080000, epsilon 0.020, loss 0.00079\n",
      "running reward: 4.79 at episode 17539, frame count 5090000, epsilon 0.020, loss 0.00139\n",
      "running reward: 5.06 at episode 17552, frame count 5100000, epsilon 0.020, loss 0.00058\n",
      "running reward: 4.71 at episode 17565, frame count 5110000, epsilon 0.020, loss 0.00022\n",
      "running reward: 4.77 at episode 17578, frame count 5120000, epsilon 0.020, loss 0.00680\n",
      "running reward: 4.67 at episode 17590, frame count 5130000, epsilon 0.020, loss 0.00107\n",
      "running reward: 4.76 at episode 17602, frame count 5140000, epsilon 0.020, loss 0.00266\n",
      "running reward: 5.12 at episode 17611, frame count 5150000, epsilon 0.020, loss 0.00292\n",
      "running reward: 5.21 at episode 17623, frame count 5160000, epsilon 0.020, loss 0.01732\n",
      "running reward: 5.07 at episode 17640, frame count 5170000, epsilon 0.020, loss 0.00078\n",
      "running reward: 5.07 at episode 17654, frame count 5180000, epsilon 0.020, loss 0.00047\n",
      "running reward: 5.03 at episode 17666, frame count 5190000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.02 at episode 17681, frame count 5200000, epsilon 0.020, loss 0.00092\n",
      "running reward: 4.92 at episode 17697, frame count 5210000, epsilon 0.020, loss 0.00027\n",
      "running reward: 4.70 at episode 17710, frame count 5220000, epsilon 0.020, loss 0.00103\n",
      "running reward: 4.54 at episode 17723, frame count 5230000, epsilon 0.020, loss 0.00047\n",
      "running reward: 4.40 at episode 17741, frame count 5240000, epsilon 0.020, loss 0.01446\n",
      "running reward: 4.31 at episode 17755, frame count 5250000, epsilon 0.020, loss 0.00044\n",
      "running reward: 4.35 at episode 17767, frame count 5260000, epsilon 0.020, loss 0.00245\n",
      "running reward: 4.39 at episode 17782, frame count 5270000, epsilon 0.020, loss 0.00113\n",
      "running reward: 4.23 at episode 17801, frame count 5280000, epsilon 0.020, loss 0.00211\n",
      "running reward: 3.90 at episode 17819, frame count 5290000, epsilon 0.020, loss 0.01042\n",
      "running reward: 3.76 at episode 17837, frame count 5300000, epsilon 0.020, loss 0.00046\n",
      "running reward: 3.92 at episode 17851, frame count 5310000, epsilon 0.020, loss 0.00043\n",
      "running reward: 3.84 at episode 17864, frame count 5320000, epsilon 0.020, loss 0.00117\n",
      "running reward: 3.70 at episode 17880, frame count 5330000, epsilon 0.020, loss 0.00038\n",
      "running reward: 3.86 at episode 17893, frame count 5340000, epsilon 0.020, loss 0.00050\n",
      "running reward: 4.23 at episode 17904, frame count 5350000, epsilon 0.020, loss 0.00055\n",
      "running reward: 4.55 at episode 17916, frame count 5360000, epsilon 0.020, loss 0.00038\n",
      "running reward: 4.87 at episode 17926, frame count 5370000, epsilon 0.020, loss 0.00078\n",
      "running reward: 5.04 at episode 17939, frame count 5380000, epsilon 0.020, loss 0.00081\n",
      "running reward: 4.98 at episode 17953, frame count 5390000, epsilon 0.020, loss 0.00230\n",
      "running reward: 4.89 at episode 17969, frame count 5400000, epsilon 0.020, loss 0.00139\n",
      "running reward: 4.93 at episode 17982, frame count 5410000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.01 at episode 17995, frame count 5420000, epsilon 0.020, loss 0.00058\n",
      "running reward: 5.17 at episode 18005, frame count 5430000, epsilon 0.020, loss 0.00052\n",
      "running reward: 4.89 at episode 18020, frame count 5440000, epsilon 0.020, loss 0.00064\n",
      "running reward: 4.80 at episode 18031, frame count 5450000, epsilon 0.020, loss 0.00032\n",
      "running reward: 5.20 at episode 18041, frame count 5460000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.06 at episode 18057, frame count 5470000, epsilon 0.020, loss 0.00073\n",
      "running reward: 5.35 at episode 18067, frame count 5480000, epsilon 0.020, loss 0.00033\n",
      "running reward: 5.45 at episode 18077, frame count 5490000, epsilon 0.020, loss 0.00091\n",
      "running reward: 5.59 at episode 18087, frame count 5500000, epsilon 0.020, loss 0.02447\n",
      "running reward: 5.63 at episode 18097, frame count 5510000, epsilon 0.020, loss 0.00041\n",
      "running reward: 5.68 at episode 18110, frame count 5520000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.73 at episode 18123, frame count 5530000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.70 at episode 18135, frame count 5540000, epsilon 0.020, loss 0.00061\n",
      "running reward: 5.64 at episode 18146, frame count 5550000, epsilon 0.020, loss 0.00037\n",
      "running reward: 5.77 at episode 18156, frame count 5560000, epsilon 0.020, loss 0.00189\n",
      "running reward: 5.16 at episode 18177, frame count 5570000, epsilon 0.020, loss 0.00022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.07 at episode 18190, frame count 5580000, epsilon 0.020, loss 0.00044\n",
      "running reward: 5.17 at episode 18200, frame count 5590000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.27 at episode 18210, frame count 5600000, epsilon 0.020, loss 0.00063\n",
      "running reward: 5.36 at episode 18221, frame count 5610000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.41 at episode 18233, frame count 5620000, epsilon 0.020, loss 0.00060\n",
      "running reward: 5.45 at episode 18243, frame count 5630000, epsilon 0.020, loss 0.00096\n",
      "running reward: 5.49 at episode 18254, frame count 5640000, epsilon 0.020, loss 0.00317\n",
      "running reward: 5.77 at episode 18265, frame count 5650000, epsilon 0.020, loss 0.00048\n",
      "running reward: 6.11 at episode 18275, frame count 5660000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.64 at episode 18292, frame count 5670000, epsilon 0.020, loss 0.00413\n",
      "running reward: 5.81 at episode 18303, frame count 5680000, epsilon 0.020, loss 0.00044\n",
      "running reward: 5.89 at episode 18311, frame count 5690000, epsilon 0.020, loss 0.00041\n",
      "running reward: 6.04 at episode 18320, frame count 5700000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.97 at episode 18332, frame count 5710000, epsilon 0.020, loss 0.00060\n",
      "running reward: 6.06 at episode 18342, frame count 5720000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.88 at episode 18355, frame count 5730000, epsilon 0.020, loss 0.00212\n",
      "running reward: 5.95 at episode 18366, frame count 5740000, epsilon 0.020, loss 0.00052\n",
      "running reward: 5.95 at episode 18377, frame count 5750000, epsilon 0.020, loss 0.00042\n",
      "running reward: 6.09 at episode 18390, frame count 5760000, epsilon 0.020, loss 0.00319\n",
      "running reward: 6.11 at episode 18400, frame count 5770000, epsilon 0.020, loss 0.00090\n",
      "running reward: 5.79 at episode 18415, frame count 5780000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.57 at episode 18426, frame count 5790000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.57 at episode 18440, frame count 5800000, epsilon 0.020, loss 0.00049\n",
      "running reward: 5.60 at episode 18451, frame count 5810000, epsilon 0.020, loss 0.03027\n",
      "running reward: 5.51 at episode 18464, frame count 5820000, epsilon 0.020, loss 0.00107\n",
      "running reward: 5.34 at episode 18476, frame count 5830000, epsilon 0.020, loss 0.00386\n",
      "running reward: 5.42 at episode 18486, frame count 5840000, epsilon 0.020, loss 0.00126\n",
      "running reward: 5.39 at episode 18498, frame count 5850000, epsilon 0.020, loss 0.00053\n",
      "running reward: 5.49 at episode 18510, frame count 5860000, epsilon 0.020, loss 0.00049\n",
      "running reward: 5.57 at episode 18521, frame count 5870000, epsilon 0.020, loss 0.00129\n",
      "running reward: 5.52 at episode 18532, frame count 5880000, epsilon 0.020, loss 0.00400\n",
      "running reward: 5.63 at episode 18545, frame count 5890000, epsilon 0.020, loss 0.00029\n",
      "running reward: 5.76 at episode 18557, frame count 5900000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.66 at episode 18570, frame count 5910000, epsilon 0.020, loss 0.00077\n",
      "running reward: 5.37 at episode 18586, frame count 5920000, epsilon 0.020, loss 0.00019\n",
      "running reward: 5.50 at episode 18596, frame count 5930000, epsilon 0.020, loss 0.00084\n",
      "running reward: 5.61 at episode 18605, frame count 5940000, epsilon 0.020, loss 0.00029\n",
      "running reward: 5.44 at episode 18621, frame count 5950000, epsilon 0.020, loss 0.00077\n",
      "running reward: 4.95 at episode 18638, frame count 5960000, epsilon 0.020, loss 0.00044\n",
      "running reward: 4.93 at episode 18650, frame count 5970000, epsilon 0.020, loss 0.00188\n",
      "running reward: 4.46 at episode 18669, frame count 5980000, epsilon 0.020, loss 0.00054\n",
      "running reward: 4.36 at episode 18684, frame count 5990000, epsilon 0.020, loss 0.00028\n",
      "running reward: 4.00 at episode 18701, frame count 6000000, epsilon 0.020, loss 0.00030\n",
      "running reward: 3.75 at episode 18720, frame count 6010000, epsilon 0.020, loss 0.00227\n",
      "running reward: 3.48 at episode 18739, frame count 6020000, epsilon 0.020, loss 0.00019\n",
      "running reward: 3.54 at episode 18756, frame count 6030000, epsilon 0.020, loss 0.00027\n",
      "running reward: 3.45 at episode 18775, frame count 6040000, epsilon 0.020, loss 0.00047\n",
      "running reward: 3.58 at episode 18788, frame count 6050000, epsilon 0.020, loss 0.00105\n",
      "running reward: 3.52 at episode 18805, frame count 6060000, epsilon 0.020, loss 0.00088\n",
      "running reward: 3.91 at episode 18817, frame count 6070000, epsilon 0.020, loss 0.00087\n",
      "running reward: 4.11 at episode 18831, frame count 6080000, epsilon 0.020, loss 0.00043\n",
      "running reward: 4.25 at episode 18845, frame count 6090000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.43 at episode 18858, frame count 6100000, epsilon 0.020, loss 0.00066\n",
      "running reward: 4.95 at episode 18866, frame count 6110000, epsilon 0.020, loss 0.00080\n",
      "running reward: 4.93 at episode 18880, frame count 6120000, epsilon 0.020, loss 0.01502\n",
      "running reward: 5.17 at episode 18890, frame count 6130000, epsilon 0.020, loss 0.00061\n",
      "running reward: 5.48 at episode 18900, frame count 6140000, epsilon 0.020, loss 0.00062\n",
      "running reward: 5.30 at episode 18918, frame count 6150000, epsilon 0.020, loss 0.00116\n",
      "running reward: 5.19 at episode 18933, frame count 6160000, epsilon 0.020, loss 0.00049\n",
      "running reward: 5.45 at episode 18944, frame count 6170000, epsilon 0.020, loss 0.00093\n",
      "running reward: 5.45 at episode 18956, frame count 6180000, epsilon 0.020, loss 0.00050\n",
      "running reward: 4.83 at episode 18975, frame count 6190000, epsilon 0.020, loss 0.00096\n",
      "running reward: 4.94 at episode 18985, frame count 6200000, epsilon 0.020, loss 0.00038\n",
      "running reward: 4.68 at episode 18999, frame count 6210000, epsilon 0.020, loss 0.00102\n",
      "running reward: 4.47 at episode 19018, frame count 6220000, epsilon 0.020, loss 0.00053\n",
      "running reward: 4.02 at episode 19038, frame count 6230000, epsilon 0.020, loss 0.00044\n",
      "running reward: 3.73 at episode 19053, frame count 6240000, epsilon 0.020, loss 0.00047\n",
      "running reward: 3.78 at episode 19065, frame count 6250000, epsilon 0.020, loss 0.00073\n",
      "running reward: 3.76 at episode 19080, frame count 6260000, epsilon 0.020, loss 0.00099\n",
      "running reward: 3.76 at episode 19095, frame count 6270000, epsilon 0.020, loss 0.00041\n",
      "running reward: 4.09 at episode 19103, frame count 6280000, epsilon 0.020, loss 0.00049\n",
      "running reward: 4.52 at episode 19115, frame count 6290000, epsilon 0.020, loss 0.00040\n",
      "running reward: 4.66 at episode 19128, frame count 6300000, epsilon 0.020, loss 0.00053\n",
      "running reward: 4.64 at episode 19147, frame count 6310000, epsilon 0.020, loss 0.00095\n",
      "running reward: 4.75 at episode 19162, frame count 6320000, epsilon 0.020, loss 0.00018\n",
      "running reward: 4.68 at episode 19175, frame count 6330000, epsilon 0.020, loss 0.00108\n",
      "running reward: 4.66 at episode 19187, frame count 6340000, epsilon 0.020, loss 0.00081\n",
      "running reward: 4.70 at episode 19201, frame count 6350000, epsilon 0.020, loss 0.00041\n",
      "running reward: 4.60 at episode 19212, frame count 6360000, epsilon 0.020, loss 0.01529\n",
      "running reward: 4.69 at episode 19224, frame count 6370000, epsilon 0.020, loss 0.00322\n",
      "running reward: 4.97 at episode 19235, frame count 6380000, epsilon 0.020, loss 0.01375\n",
      "running reward: 5.31 at episode 19246, frame count 6390000, epsilon 0.020, loss 0.00054\n",
      "running reward: 5.62 at episode 19255, frame count 6400000, epsilon 0.020, loss 0.00126\n",
      "running reward: 5.52 at episode 19269, frame count 6410000, epsilon 0.020, loss 0.00047\n",
      "running reward: 5.72 at episode 19278, frame count 6420000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.89 at episode 19290, frame count 6430000, epsilon 0.020, loss 0.00043\n",
      "running reward: 6.00 at episode 19301, frame count 6440000, epsilon 0.020, loss 0.00056\n",
      "running reward: 6.04 at episode 19310, frame count 6450000, epsilon 0.020, loss 0.00052\n",
      "running reward: 6.25 at episode 19322, frame count 6460000, epsilon 0.020, loss 0.00080\n",
      "running reward: 6.36 at episode 19331, frame count 6470000, epsilon 0.020, loss 0.00435\n",
      "running reward: 6.06 at episode 19347, frame count 6480000, epsilon 0.020, loss 0.00044\n",
      "running reward: 5.93 at episode 19357, frame count 6490000, epsilon 0.020, loss 0.00068\n",
      "running reward: 5.77 at episode 19371, frame count 6500000, epsilon 0.020, loss 0.00081\n",
      "running reward: 5.50 at episode 19382, frame count 6510000, epsilon 0.020, loss 0.00151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.77 at episode 19394, frame count 6520000, epsilon 0.020, loss 0.00025\n",
      "running reward: 5.83 at episode 19404, frame count 6530000, epsilon 0.020, loss 0.00064\n",
      "running reward: 5.49 at episode 19418, frame count 6540000, epsilon 0.020, loss 0.01311\n",
      "running reward: 4.77 at episode 19437, frame count 6550000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.41 at episode 19457, frame count 6560000, epsilon 0.020, loss 0.00139\n",
      "running reward: 3.77 at episode 19481, frame count 6570000, epsilon 0.020, loss 0.00056\n",
      "running reward: 3.65 at episode 19494, frame count 6580000, epsilon 0.020, loss 0.00027\n",
      "running reward: 3.48 at episode 19507, frame count 6590000, epsilon 0.020, loss 0.00142\n",
      "running reward: 3.18 at episode 19530, frame count 6600000, epsilon 0.020, loss 0.00063\n",
      "running reward: 3.45 at episode 19538, frame count 6610000, epsilon 0.020, loss 0.00043\n",
      "running reward: 3.73 at episode 19552, frame count 6620000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.11 at episode 19565, frame count 6630000, epsilon 0.020, loss 0.00082\n",
      "running reward: 4.53 at episode 19575, frame count 6640000, epsilon 0.020, loss 0.00036\n",
      "running reward: 4.65 at episode 19590, frame count 6650000, epsilon 0.020, loss 0.00141\n",
      "running reward: 4.69 at episode 19603, frame count 6660000, epsilon 0.020, loss 0.00783\n",
      "running reward: 5.01 at episode 19612, frame count 6670000, epsilon 0.020, loss 0.00049\n",
      "running reward: 5.20 at episode 19630, frame count 6680000, epsilon 0.020, loss 0.00368\n",
      "running reward: 5.09 at episode 19641, frame count 6690000, epsilon 0.020, loss 0.00053\n",
      "running reward: 4.94 at episode 19656, frame count 6700000, epsilon 0.020, loss 0.00043\n",
      "running reward: 4.16 at episode 19677, frame count 6710000, epsilon 0.020, loss 0.00047\n",
      "running reward: 4.59 at episode 19687, frame count 6720000, epsilon 0.020, loss 0.00099\n",
      "running reward: 4.49 at episode 19700, frame count 6730000, epsilon 0.020, loss 0.00096\n",
      "running reward: 4.61 at episode 19710, frame count 6740000, epsilon 0.020, loss 0.00123\n",
      "running reward: 4.88 at episode 19721, frame count 6750000, epsilon 0.020, loss 0.00036\n",
      "running reward: 4.57 at episode 19740, frame count 6760000, epsilon 0.020, loss 0.00321\n",
      "running reward: 4.35 at episode 19759, frame count 6770000, epsilon 0.020, loss 0.00021\n",
      "running reward: 4.52 at episode 19773, frame count 6780000, epsilon 0.020, loss 0.00054\n",
      "running reward: 4.66 at episode 19786, frame count 6790000, epsilon 0.020, loss 0.00070\n",
      "running reward: 4.52 at episode 19799, frame count 6800000, epsilon 0.020, loss 0.00153\n",
      "running reward: 4.39 at episode 19810, frame count 6810000, epsilon 0.020, loss 0.00251\n",
      "running reward: 4.15 at episode 19828, frame count 6820000, epsilon 0.020, loss 0.00049\n",
      "running reward: 4.42 at episode 19842, frame count 6830000, epsilon 0.020, loss 0.01064\n",
      "running reward: 4.58 at episode 19855, frame count 6840000, epsilon 0.020, loss 0.00501\n",
      "running reward: 5.08 at episode 19863, frame count 6850000, epsilon 0.020, loss 0.00046\n",
      "running reward: 5.15 at episode 19874, frame count 6860000, epsilon 0.020, loss 0.01120\n",
      "running reward: 5.23 at episode 19884, frame count 6870000, epsilon 0.020, loss 0.00309\n",
      "running reward: 5.41 at episode 19896, frame count 6880000, epsilon 0.020, loss 0.00064\n",
      "running reward: 5.46 at episode 19907, frame count 6890000, epsilon 0.020, loss 0.00164\n",
      "running reward: 5.74 at episode 19917, frame count 6900000, epsilon 0.020, loss 0.01286\n",
      "running reward: 6.09 at episode 19925, frame count 6910000, epsilon 0.020, loss 0.00187\n",
      "running reward: 6.12 at episode 19938, frame count 6920000, epsilon 0.020, loss 0.00059\n",
      "running reward: 6.29 at episode 19948, frame count 6930000, epsilon 0.020, loss 0.00645\n",
      "running reward: 6.22 at episode 19959, frame count 6940000, epsilon 0.020, loss 0.00048\n",
      "running reward: 6.29 at episode 19968, frame count 6950000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.27 at episode 19979, frame count 6960000, epsilon 0.020, loss 0.00170\n",
      "running reward: 6.37 at episode 19989, frame count 6970000, epsilon 0.020, loss 0.00034\n",
      "running reward: 6.64 at episode 19997, frame count 6980000, epsilon 0.020, loss 0.00222\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./breakout_alpha_progress_episodes_20000.44/assets\n",
      "running reward: 6.50 at episode 20010, frame count 6990000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.88 at episode 20026, frame count 7000000, epsilon 0.020, loss 0.00029\n",
      "running reward: 6.01 at episode 20036, frame count 7010000, epsilon 0.020, loss 0.00046\n",
      "running reward: 6.20 at episode 20046, frame count 7020000, epsilon 0.020, loss 0.00373\n",
      "running reward: 6.16 at episode 20057, frame count 7030000, epsilon 0.020, loss 0.00392\n",
      "running reward: 5.92 at episode 20070, frame count 7040000, epsilon 0.020, loss 0.00048\n",
      "running reward: 5.99 at episode 20080, frame count 7050000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.85 at episode 20090, frame count 7060000, epsilon 0.020, loss 0.00620\n",
      "running reward: 5.95 at episode 20100, frame count 7070000, epsilon 0.020, loss 0.00051\n",
      "running reward: 5.78 at episode 20116, frame count 7080000, epsilon 0.020, loss 0.00114\n",
      "running reward: 6.05 at episode 20129, frame count 7090000, epsilon 0.020, loss 0.01339\n",
      "running reward: 5.92 at episode 20140, frame count 7100000, epsilon 0.020, loss 0.00066\n",
      "running reward: 5.52 at episode 20156, frame count 7110000, epsilon 0.020, loss 0.00059\n",
      "running reward: 5.78 at episode 20165, frame count 7120000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.74 at episode 20177, frame count 7130000, epsilon 0.020, loss 0.00023\n",
      "running reward: 5.61 at episode 20190, frame count 7140000, epsilon 0.020, loss 0.02078\n",
      "running reward: 5.18 at episode 20202, frame count 7150000, epsilon 0.020, loss 0.00053\n",
      "running reward: 5.38 at episode 20216, frame count 7160000, epsilon 0.020, loss 0.00036\n",
      "running reward: 5.21 at episode 20232, frame count 7170000, epsilon 0.020, loss 0.00241\n",
      "running reward: 4.81 at episode 20248, frame count 7180000, epsilon 0.020, loss 0.00020\n",
      "running reward: 4.83 at episode 20259, frame count 7190000, epsilon 0.020, loss 0.00036\n",
      "running reward: 4.66 at episode 20275, frame count 7200000, epsilon 0.020, loss 0.00026\n",
      "running reward: 4.74 at episode 20285, frame count 7210000, epsilon 0.020, loss 0.00046\n",
      "running reward: 4.71 at episode 20296, frame count 7220000, epsilon 0.020, loss 0.00118\n",
      "running reward: 4.93 at episode 20305, frame count 7230000, epsilon 0.020, loss 0.00037\n",
      "running reward: 5.04 at episode 20322, frame count 7240000, epsilon 0.020, loss 0.00150\n",
      "running reward: 5.26 at episode 20333, frame count 7250000, epsilon 0.020, loss 0.00313\n",
      "running reward: 5.42 at episode 20344, frame count 7260000, epsilon 0.020, loss 0.00061\n",
      "running reward: 5.57 at episode 20355, frame count 7270000, epsilon 0.020, loss 0.00320\n",
      "running reward: 5.70 at episode 20365, frame count 7280000, epsilon 0.020, loss 0.00049\n",
      "running reward: 5.89 at episode 20378, frame count 7290000, epsilon 0.020, loss 0.00107\n",
      "running reward: 5.81 at episode 20388, frame count 7300000, epsilon 0.020, loss 0.00323\n",
      "running reward: 5.76 at episode 20401, frame count 7310000, epsilon 0.020, loss 0.00315\n",
      "running reward: 5.57 at episode 20413, frame count 7320000, epsilon 0.020, loss 0.00168\n",
      "running reward: 6.10 at episode 20423, frame count 7330000, epsilon 0.020, loss 0.00017\n",
      "running reward: 5.94 at episode 20435, frame count 7340000, epsilon 0.020, loss 0.00227\n",
      "running reward: 6.06 at episode 20445, frame count 7350000, epsilon 0.020, loss 0.00069\n",
      "running reward: 6.04 at episode 20456, frame count 7360000, epsilon 0.020, loss 0.00032\n",
      "running reward: 5.91 at episode 20466, frame count 7370000, epsilon 0.020, loss 0.00051\n",
      "running reward: 5.91 at episode 20480, frame count 7380000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.64 at episode 20491, frame count 7390000, epsilon 0.020, loss 0.00031\n",
      "running reward: 5.80 at episode 20502, frame count 7400000, epsilon 0.020, loss 0.00043\n",
      "running reward: 6.01 at episode 20513, frame count 7410000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.53 at episode 20527, frame count 7420000, epsilon 0.020, loss 0.00059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.44 at episode 20541, frame count 7430000, epsilon 0.020, loss 0.00031\n",
      "running reward: 5.64 at episode 20550, frame count 7440000, epsilon 0.020, loss 0.00094\n",
      "running reward: 5.61 at episode 20562, frame count 7450000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.46 at episode 20571, frame count 7460000, epsilon 0.020, loss 0.00036\n",
      "running reward: 5.36 at episode 20587, frame count 7470000, epsilon 0.020, loss 0.00045\n",
      "running reward: 5.53 at episode 20597, frame count 7480000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.36 at episode 20610, frame count 7490000, epsilon 0.020, loss 0.00046\n",
      "running reward: 5.37 at episode 20622, frame count 7500000, epsilon 0.020, loss 0.00057\n",
      "running reward: 5.69 at episode 20630, frame count 7510000, epsilon 0.020, loss 0.00134\n",
      "running reward: 5.97 at episode 20640, frame count 7520000, epsilon 0.020, loss 0.00093\n",
      "running reward: 5.72 at episode 20653, frame count 7530000, epsilon 0.020, loss 0.00231\n",
      "running reward: 5.96 at episode 20662, frame count 7540000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.84 at episode 20673, frame count 7550000, epsilon 0.020, loss 0.00060\n",
      "running reward: 6.22 at episode 20681, frame count 7560000, epsilon 0.020, loss 0.00067\n",
      "running reward: 5.67 at episode 20702, frame count 7570000, epsilon 0.020, loss 0.00104\n",
      "running reward: 5.17 at episode 20721, frame count 7580000, epsilon 0.020, loss 0.00084\n",
      "running reward: 5.15 at episode 20730, frame count 7590000, epsilon 0.020, loss 0.03047\n",
      "running reward: 4.96 at episode 20741, frame count 7600000, epsilon 0.020, loss 0.00025\n",
      "running reward: 5.00 at episode 20754, frame count 7610000, epsilon 0.020, loss 0.00371\n",
      "running reward: 4.49 at episode 20772, frame count 7620000, epsilon 0.020, loss 0.00237\n",
      "running reward: 4.26 at episode 20787, frame count 7630000, epsilon 0.020, loss 0.00049\n",
      "running reward: 4.38 at episode 20802, frame count 7640000, epsilon 0.020, loss 0.00111\n",
      "running reward: 4.69 at episode 20811, frame count 7650000, epsilon 0.020, loss 0.02777\n",
      "running reward: 4.28 at episode 20830, frame count 7660000, epsilon 0.020, loss 0.00063\n",
      "running reward: 4.22 at episode 20845, frame count 7670000, epsilon 0.020, loss 0.00024\n",
      "running reward: 4.23 at episode 20855, frame count 7680000, epsilon 0.020, loss 0.02775\n",
      "running reward: 4.31 at episode 20871, frame count 7690000, epsilon 0.020, loss 0.00017\n",
      "running reward: 4.45 at episode 20881, frame count 7700000, epsilon 0.020, loss 0.00048\n",
      "running reward: 4.68 at episode 20897, frame count 7710000, epsilon 0.020, loss 0.00567\n",
      "running reward: 4.84 at episode 20907, frame count 7720000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.69 at episode 20920, frame count 7730000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.15 at episode 20930, frame count 7740000, epsilon 0.020, loss 0.00518\n",
      "running reward: 5.31 at episode 20940, frame count 7750000, epsilon 0.020, loss 0.01897\n",
      "running reward: 5.26 at episode 20952, frame count 7760000, epsilon 0.020, loss 0.00024\n",
      "running reward: 5.47 at episode 20961, frame count 7770000, epsilon 0.020, loss 0.00088\n",
      "running reward: 5.65 at episode 20975, frame count 7780000, epsilon 0.020, loss 0.00151\n",
      "running reward: 5.51 at episode 20987, frame count 7790000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.92 at episode 20996, frame count 7800000, epsilon 0.020, loss 0.00176\n",
      "running reward: 5.46 at episode 21013, frame count 7810000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.37 at episode 21026, frame count 7820000, epsilon 0.020, loss 0.02785\n",
      "running reward: 5.08 at episode 21042, frame count 7830000, epsilon 0.020, loss 0.02197\n",
      "running reward: 4.98 at episode 21055, frame count 7840000, epsilon 0.020, loss 0.00070\n",
      "running reward: 4.78 at episode 21068, frame count 7850000, epsilon 0.020, loss 0.00088\n",
      "running reward: 4.64 at episode 21084, frame count 7860000, epsilon 0.020, loss 0.00290\n",
      "running reward: 4.24 at episode 21100, frame count 7870000, epsilon 0.020, loss 0.00024\n",
      "running reward: 4.55 at episode 21111, frame count 7880000, epsilon 0.020, loss 0.00046\n",
      "running reward: 4.84 at episode 21120, frame count 7890000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.78 at episode 21131, frame count 7900000, epsilon 0.020, loss 0.00013\n",
      "running reward: 4.97 at episode 21145, frame count 7910000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.58 at episode 21164, frame count 7920000, epsilon 0.020, loss 0.00199\n",
      "running reward: 4.72 at episode 21176, frame count 7930000, epsilon 0.020, loss 0.00158\n",
      "running reward: 5.06 at episode 21185, frame count 7940000, epsilon 0.020, loss 0.00048\n",
      "running reward: 5.49 at episode 21195, frame count 7950000, epsilon 0.020, loss 0.00097\n",
      "running reward: 5.63 at episode 21205, frame count 7960000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.72 at episode 21213, frame count 7970000, epsilon 0.020, loss 0.00041\n",
      "running reward: 5.63 at episode 21226, frame count 7980000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.76 at episode 21238, frame count 7990000, epsilon 0.020, loss 0.01086\n",
      "running reward: 5.69 at episode 21252, frame count 8000000, epsilon 0.020, loss 0.00103\n",
      "running reward: 5.98 at episode 21265, frame count 8010000, epsilon 0.020, loss 0.00073\n",
      "running reward: 6.01 at episode 21277, frame count 8020000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.73 at episode 21289, frame count 8030000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.41 at episode 21302, frame count 8040000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.49 at episode 21311, frame count 8050000, epsilon 0.020, loss 0.00379\n",
      "running reward: 5.27 at episode 21325, frame count 8060000, epsilon 0.020, loss 0.01298\n",
      "running reward: 5.24 at episode 21336, frame count 8070000, epsilon 0.020, loss 0.00505\n",
      "running reward: 5.33 at episode 21349, frame count 8080000, epsilon 0.020, loss 0.00020\n",
      "running reward: 5.26 at episode 21363, frame count 8090000, epsilon 0.020, loss 0.01596\n",
      "running reward: 5.09 at episode 21376, frame count 8100000, epsilon 0.020, loss 0.00297\n",
      "running reward: 5.07 at episode 21390, frame count 8110000, epsilon 0.020, loss 0.01125\n",
      "running reward: 5.16 at episode 21401, frame count 8120000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.86 at episode 21414, frame count 8130000, epsilon 0.020, loss 0.01069\n",
      "running reward: 4.85 at episode 21428, frame count 8140000, epsilon 0.020, loss 0.00105\n",
      "running reward: 4.81 at episode 21439, frame count 8150000, epsilon 0.020, loss 0.00053\n",
      "running reward: 4.98 at episode 21450, frame count 8160000, epsilon 0.020, loss 0.00029\n",
      "running reward: 5.10 at episode 21462, frame count 8170000, epsilon 0.020, loss 0.00054\n",
      "running reward: 5.26 at episode 21474, frame count 8180000, epsilon 0.020, loss 0.00235\n",
      "running reward: 5.29 at episode 21486, frame count 8190000, epsilon 0.020, loss 0.00243\n",
      "running reward: 5.02 at episode 21501, frame count 8200000, epsilon 0.020, loss 0.00031\n",
      "running reward: 4.91 at episode 21516, frame count 8210000, epsilon 0.020, loss 0.00170\n",
      "running reward: 4.71 at episode 21533, frame count 8220000, epsilon 0.020, loss 0.00040\n",
      "running reward: 4.57 at episode 21546, frame count 8230000, epsilon 0.020, loss 0.00104\n",
      "running reward: 4.70 at episode 21557, frame count 8240000, epsilon 0.020, loss 0.00260\n",
      "running reward: 4.83 at episode 21567, frame count 8250000, epsilon 0.020, loss 0.00165\n",
      "running reward: 4.88 at episode 21581, frame count 8260000, epsilon 0.020, loss 0.00050\n",
      "running reward: 4.81 at episode 21595, frame count 8270000, epsilon 0.020, loss 0.00084\n",
      "running reward: 4.88 at episode 21609, frame count 8280000, epsilon 0.020, loss 0.00031\n",
      "running reward: 5.15 at episode 21620, frame count 8290000, epsilon 0.020, loss 0.00915\n",
      "running reward: 5.10 at episode 21636, frame count 8300000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.07 at episode 21648, frame count 8310000, epsilon 0.020, loss 0.00048\n",
      "running reward: 5.17 at episode 21658, frame count 8320000, epsilon 0.020, loss 0.00082\n",
      "running reward: 5.07 at episode 21668, frame count 8330000, epsilon 0.020, loss 0.01118\n",
      "running reward: 5.02 at episode 21683, frame count 8340000, epsilon 0.020, loss 0.00266\n",
      "running reward: 5.21 at episode 21693, frame count 8350000, epsilon 0.020, loss 0.00020\n",
      "running reward: 5.49 at episode 21702, frame count 8360000, epsilon 0.020, loss 0.00420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.28 at episode 21713, frame count 8370000, epsilon 0.020, loss 0.00102\n",
      "running reward: 5.51 at episode 21725, frame count 8380000, epsilon 0.020, loss 0.00025\n",
      "running reward: 5.99 at episode 21733, frame count 8390000, epsilon 0.020, loss 0.00056\n",
      "running reward: 6.15 at episode 21742, frame count 8400000, epsilon 0.020, loss 0.00801\n",
      "running reward: 6.36 at episode 21752, frame count 8410000, epsilon 0.020, loss 0.00094\n",
      "running reward: 6.35 at episode 21764, frame count 8420000, epsilon 0.020, loss 0.00566\n",
      "running reward: 6.10 at episode 21775, frame count 8430000, epsilon 0.020, loss 0.00029\n",
      "running reward: 6.21 at episode 21786, frame count 8440000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.45 at episode 21798, frame count 8450000, epsilon 0.020, loss 0.00103\n",
      "running reward: 6.27 at episode 21809, frame count 8460000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.13 at episode 21821, frame count 8470000, epsilon 0.020, loss 0.00035\n",
      "running reward: 6.18 at episode 21833, frame count 8480000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.36 at episode 21851, frame count 8490000, epsilon 0.020, loss 0.00013\n",
      "running reward: 5.55 at episode 21861, frame count 8500000, epsilon 0.020, loss 0.00063\n",
      "running reward: 5.67 at episode 21870, frame count 8510000, epsilon 0.020, loss 0.00249\n",
      "running reward: 5.20 at episode 21887, frame count 8520000, epsilon 0.020, loss 0.00073\n",
      "running reward: 5.09 at episode 21899, frame count 8530000, epsilon 0.020, loss 0.00046\n",
      "running reward: 4.84 at episode 21914, frame count 8540000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.19 at episode 21939, frame count 8550000, epsilon 0.020, loss 0.00162\n",
      "running reward: 3.95 at episode 21956, frame count 8560000, epsilon 0.020, loss 0.00100\n",
      "running reward: 3.99 at episode 21966, frame count 8570000, epsilon 0.020, loss 0.00029\n",
      "running reward: 4.19 at episode 21975, frame count 8580000, epsilon 0.020, loss 0.00036\n",
      "running reward: 4.66 at episode 21986, frame count 8590000, epsilon 0.020, loss 0.00585\n",
      "running reward: 4.43 at episode 22000, frame count 8600000, epsilon 0.020, loss 0.00023\n",
      "running reward: 4.54 at episode 22012, frame count 8610000, epsilon 0.020, loss 0.00098\n",
      "running reward: 4.70 at episode 22020, frame count 8620000, epsilon 0.020, loss 0.00079\n",
      "running reward: 5.36 at episode 22031, frame count 8630000, epsilon 0.020, loss 0.00021\n",
      "running reward: 5.82 at episode 22042, frame count 8640000, epsilon 0.020, loss 0.00027\n",
      "running reward: 6.34 at episode 22051, frame count 8650000, epsilon 0.020, loss 0.00145\n",
      "running reward: 6.03 at episode 22066, frame count 8660000, epsilon 0.020, loss 0.00377\n",
      "running reward: 5.73 at episode 22076, frame count 8670000, epsilon 0.020, loss 0.00124\n",
      "running reward: 5.78 at episode 22087, frame count 8680000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.89 at episode 22098, frame count 8690000, epsilon 0.020, loss 0.00038\n",
      "running reward: 6.20 at episode 22108, frame count 8700000, epsilon 0.020, loss 0.00080\n",
      "running reward: 5.78 at episode 22122, frame count 8710000, epsilon 0.020, loss 0.00237\n",
      "running reward: 5.93 at episode 22133, frame count 8720000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.83 at episode 22145, frame count 8730000, epsilon 0.020, loss 0.00087\n",
      "running reward: 5.83 at episode 22152, frame count 8740000, epsilon 0.020, loss 0.00023\n",
      "running reward: 6.07 at episode 22164, frame count 8750000, epsilon 0.020, loss 0.00108\n",
      "running reward: 5.97 at episode 22174, frame count 8760000, epsilon 0.020, loss 0.00048\n",
      "running reward: 6.26 at episode 22183, frame count 8770000, epsilon 0.020, loss 0.00102\n",
      "running reward: 6.36 at episode 22195, frame count 8780000, epsilon 0.020, loss 0.00066\n",
      "running reward: 6.16 at episode 22207, frame count 8790000, epsilon 0.020, loss 0.00042\n",
      "running reward: 6.31 at episode 22217, frame count 8800000, epsilon 0.020, loss 0.01422\n",
      "running reward: 5.90 at episode 22233, frame count 8810000, epsilon 0.020, loss 0.00712\n",
      "running reward: 5.57 at episode 22247, frame count 8820000, epsilon 0.020, loss 0.00033\n",
      "running reward: 5.70 at episode 22256, frame count 8830000, epsilon 0.020, loss 0.00048\n",
      "running reward: 5.66 at episode 22267, frame count 8840000, epsilon 0.020, loss 0.00309\n",
      "running reward: 5.62 at episode 22277, frame count 8850000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.60 at episode 22288, frame count 8860000, epsilon 0.020, loss 0.03079\n",
      "running reward: 5.77 at episode 22298, frame count 8870000, epsilon 0.020, loss 0.00232\n",
      "running reward: 6.02 at episode 22305, frame count 8880000, epsilon 0.020, loss 0.00086\n",
      "running reward: 6.08 at episode 22315, frame count 8890000, epsilon 0.020, loss 0.00027\n",
      "running reward: 6.45 at episode 22328, frame count 8900000, epsilon 0.020, loss 0.02369\n",
      "running reward: 6.32 at episode 22337, frame count 8910000, epsilon 0.020, loss 0.00038\n",
      "running reward: 6.71 at episode 22348, frame count 8920000, epsilon 0.020, loss 0.00419\n",
      "running reward: 6.37 at episode 22362, frame count 8930000, epsilon 0.020, loss 0.00029\n",
      "running reward: 6.48 at episode 22372, frame count 8940000, epsilon 0.020, loss 0.00025\n",
      "running reward: 6.18 at episode 22384, frame count 8950000, epsilon 0.020, loss 0.01725\n",
      "running reward: 6.38 at episode 22394, frame count 8960000, epsilon 0.020, loss 0.00133\n",
      "running reward: 6.16 at episode 22406, frame count 8970000, epsilon 0.020, loss 0.00044\n",
      "running reward: 5.93 at episode 22419, frame count 8980000, epsilon 0.020, loss 0.00071\n",
      "running reward: 5.94 at episode 22430, frame count 8990000, epsilon 0.020, loss 0.00033\n",
      "running reward: 6.08 at episode 22440, frame count 9000000, epsilon 0.020, loss 0.00072\n",
      "running reward: 6.24 at episode 22449, frame count 9010000, epsilon 0.020, loss 0.00148\n",
      "running reward: 6.45 at episode 22460, frame count 9020000, epsilon 0.020, loss 0.00089\n",
      "running reward: 6.50 at episode 22472, frame count 9030000, epsilon 0.020, loss 0.00020\n",
      "running reward: 6.45 at episode 22484, frame count 9040000, epsilon 0.020, loss 0.00028\n",
      "running reward: 6.12 at episode 22496, frame count 9050000, epsilon 0.020, loss 0.00106\n",
      "running reward: 6.36 at episode 22505, frame count 9060000, epsilon 0.020, loss 0.00038\n",
      "running reward: 6.73 at episode 22512, frame count 9070000, epsilon 0.020, loss 0.01440\n",
      "running reward: 6.72 at episode 22522, frame count 9080000, epsilon 0.020, loss 0.00036\n",
      "running reward: 6.60 at episode 22535, frame count 9090000, epsilon 0.020, loss 0.00031\n",
      "running reward: 6.36 at episode 22547, frame count 9100000, epsilon 0.020, loss 0.00021\n",
      "running reward: 5.54 at episode 22567, frame count 9110000, epsilon 0.020, loss 0.00061\n",
      "running reward: 5.67 at episode 22576, frame count 9120000, epsilon 0.020, loss 0.00060\n",
      "running reward: 5.75 at episode 22590, frame count 9130000, epsilon 0.020, loss 0.00200\n",
      "running reward: 5.51 at episode 22601, frame count 9140000, epsilon 0.020, loss 0.00096\n",
      "running reward: 5.02 at episode 22614, frame count 9150000, epsilon 0.020, loss 0.00102\n",
      "running reward: 5.08 at episode 22624, frame count 9160000, epsilon 0.020, loss 0.00041\n",
      "running reward: 5.01 at episode 22637, frame count 9170000, epsilon 0.020, loss 0.00069\n",
      "running reward: 5.22 at episode 22646, frame count 9180000, epsilon 0.020, loss 0.00070\n",
      "running reward: 5.62 at episode 22657, frame count 9190000, epsilon 0.020, loss 0.00060\n",
      "running reward: 5.89 at episode 22668, frame count 9200000, epsilon 0.020, loss 0.00108\n",
      "running reward: 5.75 at episode 22681, frame count 9210000, epsilon 0.020, loss 0.00140\n",
      "running reward: 5.54 at episode 22696, frame count 9220000, epsilon 0.020, loss 0.00062\n",
      "running reward: 5.59 at episode 22708, frame count 9230000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.52 at episode 22719, frame count 9240000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.43 at episode 22732, frame count 9250000, epsilon 0.020, loss 0.00050\n",
      "running reward: 5.42 at episode 22744, frame count 9260000, epsilon 0.020, loss 0.00019\n",
      "running reward: 5.31 at episode 22756, frame count 9270000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.29 at episode 22767, frame count 9280000, epsilon 0.020, loss 0.00020\n",
      "running reward: 5.47 at episode 22778, frame count 9290000, epsilon 0.020, loss 0.00024\n",
      "running reward: 5.57 at episode 22790, frame count 9300000, epsilon 0.020, loss 0.00038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.56 at episode 22802, frame count 9310000, epsilon 0.020, loss 0.00197\n",
      "running reward: 5.29 at episode 22817, frame count 9320000, epsilon 0.020, loss 0.00747\n",
      "running reward: 5.39 at episode 22829, frame count 9330000, epsilon 0.020, loss 0.00037\n",
      "running reward: 5.43 at episode 22841, frame count 9340000, epsilon 0.020, loss 0.00031\n",
      "running reward: 5.37 at episode 22852, frame count 9350000, epsilon 0.020, loss 0.00020\n",
      "running reward: 5.59 at episode 22863, frame count 9360000, epsilon 0.020, loss 0.00013\n",
      "running reward: 5.55 at episode 22875, frame count 9370000, epsilon 0.020, loss 0.00064\n",
      "running reward: 5.51 at episode 22886, frame count 9380000, epsilon 0.020, loss 0.00181\n",
      "running reward: 5.82 at episode 22895, frame count 9390000, epsilon 0.020, loss 0.00500\n",
      "running reward: 5.85 at episode 22906, frame count 9400000, epsilon 0.020, loss 0.00017\n",
      "running reward: 6.23 at episode 22914, frame count 9410000, epsilon 0.020, loss 0.00024\n",
      "running reward: 6.01 at episode 22928, frame count 9420000, epsilon 0.020, loss 0.00709\n",
      "running reward: 5.90 at episode 22940, frame count 9430000, epsilon 0.020, loss 0.00108\n",
      "running reward: 5.44 at episode 22956, frame count 9440000, epsilon 0.020, loss 0.00062\n",
      "running reward: 5.40 at episode 22968, frame count 9450000, epsilon 0.020, loss 0.00158\n",
      "running reward: 5.48 at episode 22978, frame count 9460000, epsilon 0.020, loss 0.00027\n",
      "running reward: 5.41 at episode 22989, frame count 9470000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.21 at episode 23001, frame count 9480000, epsilon 0.020, loss 0.00019\n",
      "running reward: 5.02 at episode 23013, frame count 9490000, epsilon 0.020, loss 0.00359\n",
      "running reward: 5.38 at episode 23023, frame count 9500000, epsilon 0.020, loss 0.00108\n",
      "running reward: 5.52 at episode 23032, frame count 9510000, epsilon 0.020, loss 0.00050\n",
      "running reward: 5.77 at episode 23041, frame count 9520000, epsilon 0.020, loss 0.02263\n",
      "running reward: 5.99 at episode 23051, frame count 9530000, epsilon 0.020, loss 0.00248\n",
      "running reward: 6.34 at episode 23060, frame count 9540000, epsilon 0.020, loss 0.00303\n",
      "running reward: 6.44 at episode 23071, frame count 9550000, epsilon 0.020, loss 0.00210\n",
      "running reward: 6.43 at episode 23081, frame count 9560000, epsilon 0.020, loss 0.00479\n",
      "running reward: 6.75 at episode 23090, frame count 9570000, epsilon 0.020, loss 0.00067\n",
      "running reward: 6.96 at episode 23097, frame count 9580000, epsilon 0.020, loss 0.00123\n",
      "running reward: 7.11 at episode 23106, frame count 9590000, epsilon 0.020, loss 0.00047\n",
      "running reward: 6.98 at episode 23120, frame count 9600000, epsilon 0.020, loss 0.00058\n",
      "running reward: 7.31 at episode 23129, frame count 9610000, epsilon 0.020, loss 0.00080\n",
      "running reward: 7.02 at episode 23139, frame count 9620000, epsilon 0.020, loss 0.00360\n",
      "running reward: 6.97 at episode 23151, frame count 9630000, epsilon 0.020, loss 0.00027\n",
      "running reward: 6.78 at episode 23160, frame count 9640000, epsilon 0.020, loss 0.00168\n",
      "running reward: 6.51 at episode 23174, frame count 9650000, epsilon 0.020, loss 0.00072\n",
      "running reward: 6.51 at episode 23183, frame count 9660000, epsilon 0.020, loss 0.00234\n",
      "running reward: 6.15 at episode 23196, frame count 9670000, epsilon 0.020, loss 0.00046\n",
      "running reward: 6.07 at episode 23207, frame count 9680000, epsilon 0.020, loss 0.00056\n",
      "running reward: 6.31 at episode 23216, frame count 9690000, epsilon 0.020, loss 0.00030\n",
      "running reward: 6.25 at episode 23225, frame count 9700000, epsilon 0.020, loss 0.00066\n",
      "running reward: 6.30 at episode 23235, frame count 9710000, epsilon 0.020, loss 0.00024\n",
      "running reward: 6.44 at episode 23242, frame count 9720000, epsilon 0.020, loss 0.00091\n",
      "running reward: 6.19 at episode 23257, frame count 9730000, epsilon 0.020, loss 0.00304\n",
      "running reward: 6.34 at episode 23268, frame count 9740000, epsilon 0.020, loss 0.00316\n",
      "running reward: 6.26 at episode 23281, frame count 9750000, epsilon 0.020, loss 0.00039\n",
      "running reward: 6.30 at episode 23292, frame count 9760000, epsilon 0.020, loss 0.00020\n",
      "running reward: 6.22 at episode 23302, frame count 9770000, epsilon 0.020, loss 0.00024\n",
      "running reward: 6.28 at episode 23312, frame count 9780000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.76 at episode 23327, frame count 9790000, epsilon 0.020, loss 0.00023\n",
      "running reward: 5.36 at episode 23341, frame count 9800000, epsilon 0.020, loss 0.00100\n",
      "running reward: 5.47 at episode 23352, frame count 9810000, epsilon 0.020, loss 0.00400\n",
      "running reward: 5.36 at episode 23362, frame count 9820000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.62 at episode 23376, frame count 9830000, epsilon 0.020, loss 0.00045\n",
      "running reward: 5.52 at episode 23387, frame count 9840000, epsilon 0.020, loss 0.00081\n",
      "running reward: 5.27 at episode 23402, frame count 9850000, epsilon 0.020, loss 0.00065\n",
      "running reward: 4.87 at episode 23418, frame count 9860000, epsilon 0.020, loss 0.00014\n",
      "running reward: 4.98 at episode 23431, frame count 9870000, epsilon 0.020, loss 0.00112\n",
      "running reward: 5.15 at episode 23442, frame count 9880000, epsilon 0.020, loss 0.00019\n",
      "running reward: 4.97 at episode 23455, frame count 9890000, epsilon 0.020, loss 0.00051\n",
      "running reward: 4.98 at episode 23465, frame count 9900000, epsilon 0.020, loss 0.00082\n",
      "running reward: 5.19 at episode 23477, frame count 9910000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.15 at episode 23489, frame count 9920000, epsilon 0.020, loss 0.00017\n",
      "running reward: 5.25 at episode 23500, frame count 9930000, epsilon 0.020, loss 0.00055\n",
      "running reward: 5.56 at episode 23513, frame count 9940000, epsilon 0.020, loss 0.01917\n",
      "running reward: 5.62 at episode 23525, frame count 9950000, epsilon 0.020, loss 0.00067\n",
      "running reward: 5.41 at episode 23540, frame count 9960000, epsilon 0.020, loss 0.00042\n",
      "running reward: 5.56 at episode 23550, frame count 9970000, epsilon 0.020, loss 0.00071\n",
      "running reward: 5.70 at episode 23562, frame count 9980000, epsilon 0.020, loss 0.00149\n",
      "running reward: 5.70 at episode 23573, frame count 9990000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.60 at episode 23586, frame count 10000000, epsilon 0.020, loss 0.00073\n",
      "running reward: 5.56 at episode 23596, frame count 10010000, epsilon 0.020, loss 0.00423\n",
      "running reward: 5.67 at episode 23608, frame count 10020000, epsilon 0.020, loss 0.00211\n",
      "running reward: 5.89 at episode 23620, frame count 10030000, epsilon 0.020, loss 0.02414\n",
      "running reward: 6.04 at episode 23631, frame count 10040000, epsilon 0.020, loss 0.00046\n",
      "running reward: 5.93 at episode 23644, frame count 10050000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.82 at episode 23657, frame count 10060000, epsilon 0.020, loss 0.00152\n",
      "running reward: 5.79 at episode 23669, frame count 10070000, epsilon 0.020, loss 0.00038\n",
      "running reward: 5.65 at episode 23683, frame count 10080000, epsilon 0.020, loss 0.00093\n",
      "running reward: 5.58 at episode 23694, frame count 10090000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.52 at episode 23706, frame count 10100000, epsilon 0.020, loss 0.00025\n",
      "running reward: 5.30 at episode 23720, frame count 10110000, epsilon 0.020, loss 0.00555\n",
      "running reward: 5.23 at episode 23732, frame count 10120000, epsilon 0.020, loss 0.00305\n",
      "running reward: 5.29 at episode 23744, frame count 10130000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.42 at episode 23756, frame count 10140000, epsilon 0.020, loss 0.00020\n",
      "running reward: 5.40 at episode 23767, frame count 10150000, epsilon 0.020, loss 0.00016\n",
      "running reward: 5.60 at episode 23778, frame count 10160000, epsilon 0.020, loss 0.00092\n",
      "running reward: 5.26 at episode 23796, frame count 10170000, epsilon 0.020, loss 0.00071\n",
      "running reward: 5.17 at episode 23810, frame count 10180000, epsilon 0.020, loss 0.00073\n",
      "running reward: 5.40 at episode 23817, frame count 10190000, epsilon 0.020, loss 0.00027\n",
      "running reward: 5.57 at episode 23829, frame count 10200000, epsilon 0.020, loss 0.00269\n",
      "running reward: 5.45 at episode 23841, frame count 10210000, epsilon 0.020, loss 0.00103\n",
      "running reward: 5.72 at episode 23851, frame count 10220000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.65 at episode 23865, frame count 10230000, epsilon 0.020, loss 0.00038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.81 at episode 23872, frame count 10240000, epsilon 0.020, loss 0.00191\n",
      "running reward: 5.83 at episode 23883, frame count 10250000, epsilon 0.020, loss 0.00034\n",
      "running reward: 6.18 at episode 23895, frame count 10260000, epsilon 0.020, loss 0.00084\n",
      "running reward: 6.49 at episode 23904, frame count 10270000, epsilon 0.020, loss 0.00796\n",
      "running reward: 6.16 at episode 23916, frame count 10280000, epsilon 0.020, loss 0.00104\n",
      "running reward: 5.45 at episode 23940, frame count 10290000, epsilon 0.020, loss 0.00023\n",
      "running reward: 5.46 at episode 23949, frame count 10300000, epsilon 0.020, loss 0.00957\n",
      "running reward: 5.49 at episode 23960, frame count 10310000, epsilon 0.020, loss 0.00061\n",
      "running reward: 5.32 at episode 23973, frame count 10320000, epsilon 0.020, loss 0.01428\n",
      "running reward: 5.38 at episode 23985, frame count 10330000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.51 at episode 23992, frame count 10340000, epsilon 0.020, loss 0.02919\n",
      "running reward: 5.47 at episode 24002, frame count 10350000, epsilon 0.020, loss 0.00156\n",
      "running reward: 5.77 at episode 24011, frame count 10360000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.91 at episode 24021, frame count 10370000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.35 at episode 24033, frame count 10380000, epsilon 0.020, loss 0.01497\n",
      "running reward: 6.72 at episode 24041, frame count 10390000, epsilon 0.020, loss 0.00063\n",
      "running reward: 6.45 at episode 24056, frame count 10400000, epsilon 0.020, loss 0.00023\n",
      "running reward: 6.44 at episode 24064, frame count 10410000, epsilon 0.020, loss 0.00017\n",
      "running reward: 6.29 at episode 24077, frame count 10420000, epsilon 0.020, loss 0.00028\n",
      "running reward: 6.23 at episode 24086, frame count 10430000, epsilon 0.020, loss 0.00093\n",
      "running reward: 6.24 at episode 24098, frame count 10440000, epsilon 0.020, loss 0.00111\n",
      "running reward: 6.35 at episode 24108, frame count 10450000, epsilon 0.020, loss 0.00048\n",
      "running reward: 6.07 at episode 24119, frame count 10460000, epsilon 0.020, loss 0.00045\n",
      "running reward: 6.14 at episode 24128, frame count 10470000, epsilon 0.020, loss 0.00103\n",
      "running reward: 6.20 at episode 24139, frame count 10480000, epsilon 0.020, loss 0.00136\n",
      "running reward: 6.19 at episode 24150, frame count 10490000, epsilon 0.020, loss 0.00326\n",
      "running reward: 6.30 at episode 24162, frame count 10500000, epsilon 0.020, loss 0.00177\n",
      "running reward: 6.28 at episode 24173, frame count 10510000, epsilon 0.020, loss 0.00036\n",
      "running reward: 6.25 at episode 24185, frame count 10520000, epsilon 0.020, loss 0.00128\n",
      "running reward: 6.00 at episode 24200, frame count 10530000, epsilon 0.020, loss 0.00196\n",
      "running reward: 5.73 at episode 24213, frame count 10540000, epsilon 0.020, loss 0.00079\n",
      "running reward: 5.57 at episode 24226, frame count 10550000, epsilon 0.020, loss 0.00058\n",
      "running reward: 5.18 at episode 24238, frame count 10560000, epsilon 0.020, loss 0.00054\n",
      "running reward: 5.42 at episode 24249, frame count 10570000, epsilon 0.020, loss 0.00206\n",
      "running reward: 5.52 at episode 24260, frame count 10580000, epsilon 0.020, loss 0.00086\n",
      "running reward: 5.24 at episode 24273, frame count 10590000, epsilon 0.020, loss 0.00045\n",
      "running reward: 5.58 at episode 24281, frame count 10600000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.97 at episode 24290, frame count 10610000, epsilon 0.020, loss 0.03818\n",
      "running reward: 6.09 at episode 24300, frame count 10620000, epsilon 0.020, loss 0.00104\n",
      "running reward: 6.39 at episode 24310, frame count 10630000, epsilon 0.020, loss 0.00322\n",
      "running reward: 6.22 at episode 24323, frame count 10640000, epsilon 0.020, loss 0.00147\n",
      "running reward: 6.46 at episode 24335, frame count 10650000, epsilon 0.020, loss 0.00053\n",
      "running reward: 6.30 at episode 24347, frame count 10660000, epsilon 0.020, loss 0.00119\n",
      "running reward: 6.17 at episode 24359, frame count 10670000, epsilon 0.020, loss 0.00091\n",
      "running reward: 6.20 at episode 24372, frame count 10680000, epsilon 0.020, loss 0.00055\n",
      "running reward: 6.17 at episode 24380, frame count 10690000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.03 at episode 24390, frame count 10700000, epsilon 0.020, loss 0.00899\n",
      "running reward: 6.08 at episode 24398, frame count 10710000, epsilon 0.020, loss 0.00061\n",
      "running reward: 6.08 at episode 24406, frame count 10720000, epsilon 0.020, loss 0.00095\n",
      "running reward: 6.31 at episode 24416, frame count 10730000, epsilon 0.020, loss 0.00062\n",
      "running reward: 6.55 at episode 24425, frame count 10740000, epsilon 0.020, loss 0.00110\n",
      "running reward: 6.49 at episode 24437, frame count 10750000, epsilon 0.020, loss 0.00072\n",
      "running reward: 6.69 at episode 24446, frame count 10760000, epsilon 0.020, loss 0.00156\n",
      "running reward: 6.71 at episode 24457, frame count 10770000, epsilon 0.020, loss 0.00032\n",
      "running reward: 6.84 at episode 24466, frame count 10780000, epsilon 0.020, loss 0.00099\n",
      "running reward: 7.07 at episode 24476, frame count 10790000, epsilon 0.020, loss 0.00100\n",
      "running reward: 7.01 at episode 24485, frame count 10800000, epsilon 0.020, loss 0.00072\n",
      "running reward: 6.69 at episode 24498, frame count 10810000, epsilon 0.020, loss 0.00028\n",
      "running reward: 6.44 at episode 24509, frame count 10820000, epsilon 0.020, loss 0.00082\n",
      "running reward: 6.39 at episode 24520, frame count 10830000, epsilon 0.020, loss 0.00017\n",
      "running reward: 6.45 at episode 24530, frame count 10840000, epsilon 0.020, loss 0.00035\n",
      "running reward: 6.22 at episode 24542, frame count 10850000, epsilon 0.020, loss 0.00052\n",
      "running reward: 6.33 at episode 24554, frame count 10860000, epsilon 0.020, loss 0.00037\n",
      "running reward: 6.22 at episode 24565, frame count 10870000, epsilon 0.020, loss 0.00091\n",
      "running reward: 6.11 at episode 24576, frame count 10880000, epsilon 0.020, loss 0.00274\n",
      "running reward: 5.97 at episode 24586, frame count 10890000, epsilon 0.020, loss 0.01491\n",
      "running reward: 6.12 at episode 24598, frame count 10900000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.25 at episode 24606, frame count 10910000, epsilon 0.020, loss 0.00096\n",
      "running reward: 6.44 at episode 24616, frame count 10920000, epsilon 0.020, loss 0.00159\n",
      "running reward: 6.22 at episode 24627, frame count 10930000, epsilon 0.020, loss 0.00030\n",
      "running reward: 6.36 at episode 24638, frame count 10940000, epsilon 0.020, loss 0.00082\n",
      "running reward: 6.48 at episode 24646, frame count 10950000, epsilon 0.020, loss 0.00047\n",
      "running reward: 6.72 at episode 24656, frame count 10960000, epsilon 0.020, loss 0.02103\n",
      "running reward: 6.17 at episode 24676, frame count 10970000, epsilon 0.020, loss 0.00029\n",
      "running reward: 5.89 at episode 24691, frame count 10980000, epsilon 0.020, loss 0.00203\n",
      "running reward: 5.97 at episode 24701, frame count 10990000, epsilon 0.020, loss 0.00042\n",
      "running reward: 5.43 at episode 24715, frame count 11000000, epsilon 0.020, loss 0.00055\n",
      "running reward: 5.21 at episode 24730, frame count 11010000, epsilon 0.020, loss 0.00069\n",
      "running reward: 5.17 at episode 24741, frame count 11020000, epsilon 0.020, loss 0.00068\n",
      "running reward: 4.77 at episode 24754, frame count 11030000, epsilon 0.020, loss 0.00032\n",
      "running reward: 4.75 at episode 24762, frame count 11040000, epsilon 0.020, loss 0.00083\n",
      "running reward: 5.25 at episode 24771, frame count 11050000, epsilon 0.020, loss 0.00050\n",
      "running reward: 5.68 at episode 24781, frame count 11060000, epsilon 0.020, loss 0.00058\n",
      "running reward: 5.88 at episode 24793, frame count 11070000, epsilon 0.020, loss 0.00045\n",
      "running reward: 5.73 at episode 24805, frame count 11080000, epsilon 0.020, loss 0.00057\n",
      "running reward: 5.99 at episode 24819, frame count 11090000, epsilon 0.020, loss 0.00695\n",
      "running reward: 5.93 at episode 24831, frame count 11100000, epsilon 0.020, loss 0.00438\n",
      "running reward: 5.83 at episode 24844, frame count 11110000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.76 at episode 24859, frame count 11120000, epsilon 0.020, loss 0.00062\n",
      "running reward: 5.61 at episode 24869, frame count 11130000, epsilon 0.020, loss 0.00087\n",
      "running reward: 5.57 at episode 24880, frame count 11140000, epsilon 0.020, loss 0.00287\n",
      "running reward: 5.48 at episode 24891, frame count 11150000, epsilon 0.020, loss 0.00068\n",
      "running reward: 5.02 at episode 24908, frame count 11160000, epsilon 0.020, loss 0.00015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.03 at episode 24923, frame count 11170000, epsilon 0.020, loss 0.00070\n",
      "running reward: 4.70 at episode 24940, frame count 11180000, epsilon 0.020, loss 0.00089\n",
      "running reward: 4.70 at episode 24953, frame count 11190000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.92 at episode 24961, frame count 11200000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.71 at episode 24976, frame count 11210000, epsilon 0.020, loss 0.00062\n",
      "running reward: 4.81 at episode 24986, frame count 11220000, epsilon 0.020, loss 0.00103\n",
      "running reward: 5.04 at episode 24997, frame count 11230000, epsilon 0.020, loss 0.00102\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: ./breakout_alpha_progress_episodes_25000.44/assets\n",
      "running reward: 5.46 at episode 25005, frame count 11240000, epsilon 0.020, loss 0.00295\n",
      "running reward: 5.74 at episode 25012, frame count 11250000, epsilon 0.020, loss 0.00084\n",
      "running reward: 5.92 at episode 25024, frame count 11260000, epsilon 0.020, loss 0.00061\n",
      "running reward: 6.17 at episode 25035, frame count 11270000, epsilon 0.020, loss 0.00027\n",
      "running reward: 6.38 at episode 25047, frame count 11280000, epsilon 0.020, loss 0.00059\n",
      "running reward: 6.20 at episode 25060, frame count 11290000, epsilon 0.020, loss 0.00070\n",
      "running reward: 6.36 at episode 25069, frame count 11300000, epsilon 0.020, loss 0.00021\n",
      "running reward: 6.21 at episode 25083, frame count 11310000, epsilon 0.020, loss 0.00047\n",
      "running reward: 6.05 at episode 25094, frame count 11320000, epsilon 0.020, loss 0.00067\n",
      "running reward: 6.23 at episode 25101, frame count 11330000, epsilon 0.020, loss 0.00179\n",
      "running reward: 6.21 at episode 25109, frame count 11340000, epsilon 0.020, loss 0.00018\n",
      "running reward: 6.31 at episode 25116, frame count 11350000, epsilon 0.020, loss 0.00022\n",
      "running reward: 6.17 at episode 25130, frame count 11360000, epsilon 0.020, loss 0.00045\n",
      "running reward: 6.16 at episode 25141, frame count 11370000, epsilon 0.020, loss 0.00081\n",
      "running reward: 6.48 at episode 25150, frame count 11380000, epsilon 0.020, loss 0.02199\n",
      "running reward: 6.57 at episode 25160, frame count 11390000, epsilon 0.020, loss 0.00033\n",
      "running reward: 6.45 at episode 25170, frame count 11400000, epsilon 0.020, loss 0.00050\n",
      "running reward: 6.65 at episode 25181, frame count 11410000, epsilon 0.020, loss 0.00024\n",
      "running reward: 6.65 at episode 25192, frame count 11420000, epsilon 0.020, loss 0.00020\n",
      "running reward: 6.33 at episode 25203, frame count 11430000, epsilon 0.020, loss 0.00063\n",
      "running reward: 6.00 at episode 25216, frame count 11440000, epsilon 0.020, loss 0.00085\n",
      "running reward: 5.99 at episode 25227, frame count 11450000, epsilon 0.020, loss 0.00510\n",
      "running reward: 5.89 at episode 25241, frame count 11460000, epsilon 0.020, loss 0.00056\n",
      "running reward: 5.78 at episode 25252, frame count 11470000, epsilon 0.020, loss 0.00064\n",
      "running reward: 5.34 at episode 25268, frame count 11480000, epsilon 0.020, loss 0.00169\n",
      "running reward: 5.28 at episode 25279, frame count 11490000, epsilon 0.020, loss 0.00018\n",
      "running reward: 5.02 at episode 25293, frame count 11500000, epsilon 0.020, loss 0.00271\n",
      "running reward: 4.83 at episode 25309, frame count 11510000, epsilon 0.020, loss 0.00084\n",
      "running reward: 4.85 at episode 25320, frame count 11520000, epsilon 0.020, loss 0.00828\n",
      "running reward: 4.51 at episode 25337, frame count 11530000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.30 at episode 25353, frame count 11540000, epsilon 0.020, loss 0.00026\n",
      "running reward: 4.26 at episode 25369, frame count 11550000, epsilon 0.020, loss 0.00045\n",
      "running reward: 3.96 at episode 25387, frame count 11560000, epsilon 0.020, loss 0.00047\n",
      "running reward: 3.80 at episode 25404, frame count 11570000, epsilon 0.020, loss 0.00027\n",
      "running reward: 4.06 at episode 25416, frame count 11580000, epsilon 0.020, loss 0.00032\n",
      "running reward: 3.96 at episode 25430, frame count 11590000, epsilon 0.020, loss 0.00036\n",
      "running reward: 4.23 at episode 25442, frame count 11600000, epsilon 0.020, loss 0.00128\n",
      "running reward: 4.48 at episode 25453, frame count 11610000, epsilon 0.020, loss 0.00043\n",
      "running reward: 4.51 at episode 25468, frame count 11620000, epsilon 0.020, loss 0.00042\n",
      "running reward: 4.56 at episode 25481, frame count 11630000, epsilon 0.020, loss 0.00100\n",
      "running reward: 4.92 at episode 25495, frame count 11640000, epsilon 0.020, loss 0.02163\n",
      "running reward: 4.74 at episode 25510, frame count 11650000, epsilon 0.020, loss 0.00020\n",
      "running reward: 4.70 at episode 25522, frame count 11660000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.01 at episode 25532, frame count 11670000, epsilon 0.020, loss 0.00050\n",
      "running reward: 5.01 at episode 25545, frame count 11680000, epsilon 0.020, loss 0.00032\n",
      "running reward: 4.55 at episode 25563, frame count 11690000, epsilon 0.020, loss 0.00116\n",
      "running reward: 4.40 at episode 25579, frame count 11700000, epsilon 0.020, loss 0.00025\n",
      "running reward: 4.66 at episode 25590, frame count 11710000, epsilon 0.020, loss 0.00142\n",
      "running reward: 4.41 at episode 25606, frame count 11720000, epsilon 0.020, loss 0.00062\n",
      "running reward: 4.16 at episode 25624, frame count 11730000, epsilon 0.020, loss 0.00069\n",
      "running reward: 3.79 at episode 25644, frame count 11740000, epsilon 0.020, loss 0.00035\n",
      "running reward: 3.75 at episode 25656, frame count 11750000, epsilon 0.020, loss 0.00114\n",
      "running reward: 3.88 at episode 25672, frame count 11760000, epsilon 0.020, loss 0.00263\n",
      "running reward: 3.96 at episode 25685, frame count 11770000, epsilon 0.020, loss 0.00060\n",
      "running reward: 4.03 at episode 25697, frame count 11780000, epsilon 0.020, loss 0.00114\n",
      "running reward: 4.27 at episode 25708, frame count 11790000, epsilon 0.020, loss 0.01288\n",
      "running reward: 4.50 at episode 25721, frame count 11800000, epsilon 0.020, loss 0.01109\n",
      "running reward: 4.73 at episode 25732, frame count 11810000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.16 at episode 25741, frame count 11820000, epsilon 0.020, loss 0.00102\n",
      "running reward: 4.89 at episode 25758, frame count 11830000, epsilon 0.020, loss 0.00086\n",
      "running reward: 5.25 at episode 25767, frame count 11840000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.27 at episode 25781, frame count 11850000, epsilon 0.020, loss 0.00301\n",
      "running reward: 5.35 at episode 25792, frame count 11860000, epsilon 0.020, loss 0.01100\n",
      "running reward: 4.69 at episode 25813, frame count 11870000, epsilon 0.020, loss 0.00098\n",
      "running reward: 4.07 at episode 25836, frame count 11880000, epsilon 0.020, loss 0.00083\n",
      "running reward: 3.92 at episode 25853, frame count 11890000, epsilon 0.020, loss 0.00026\n",
      "running reward: 3.66 at episode 25868, frame count 11900000, epsilon 0.020, loss 0.00059\n",
      "running reward: 3.37 at episode 25883, frame count 11910000, epsilon 0.020, loss 0.00745\n",
      "running reward: 3.35 at episode 25900, frame count 11920000, epsilon 0.020, loss 0.00088\n",
      "running reward: 3.81 at episode 25910, frame count 11930000, epsilon 0.020, loss 0.00563\n",
      "running reward: 4.12 at episode 25925, frame count 11940000, epsilon 0.020, loss 0.00124\n",
      "running reward: 4.16 at episode 25940, frame count 11950000, epsilon 0.020, loss 0.00048\n",
      "running reward: 4.45 at episode 25950, frame count 11960000, epsilon 0.020, loss 0.00030\n",
      "running reward: 4.16 at episode 25971, frame count 11970000, epsilon 0.020, loss 0.00045\n",
      "running reward: 4.16 at episode 25986, frame count 11980000, epsilon 0.020, loss 0.00017\n",
      "running reward: 4.38 at episode 25998, frame count 11990000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.44 at episode 26009, frame count 12000000, epsilon 0.020, loss 0.00049\n",
      "running reward: 4.26 at episode 26026, frame count 12010000, epsilon 0.020, loss 0.00031\n",
      "running reward: 4.37 at episode 26039, frame count 12020000, epsilon 0.020, loss 0.00069\n",
      "running reward: 4.17 at episode 26058, frame count 12030000, epsilon 0.020, loss 0.01311\n",
      "running reward: 4.33 at episode 26072, frame count 12040000, epsilon 0.020, loss 0.00079\n",
      "running reward: 4.34 at episode 26086, frame count 12050000, epsilon 0.020, loss 0.00845\n",
      "running reward: 3.79 at episode 26107, frame count 12060000, epsilon 0.020, loss 0.00071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 3.88 at episode 26120, frame count 12070000, epsilon 0.020, loss 0.00056\n",
      "running reward: 3.81 at episode 26136, frame count 12080000, epsilon 0.020, loss 0.00090\n",
      "running reward: 3.87 at episode 26147, frame count 12090000, epsilon 0.020, loss 0.00800\n",
      "running reward: 4.32 at episode 26156, frame count 12100000, epsilon 0.020, loss 0.00150\n",
      "running reward: 4.40 at episode 26172, frame count 12110000, epsilon 0.020, loss 0.00051\n",
      "running reward: 4.51 at episode 26183, frame count 12120000, epsilon 0.020, loss 0.00418\n",
      "running reward: 4.87 at episode 26195, frame count 12130000, epsilon 0.020, loss 0.00024\n",
      "running reward: 5.16 at episode 26207, frame count 12140000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.32 at episode 26219, frame count 12150000, epsilon 0.020, loss 0.00058\n",
      "running reward: 5.55 at episode 26227, frame count 12160000, epsilon 0.020, loss 0.00084\n",
      "running reward: 5.76 at episode 26239, frame count 12170000, epsilon 0.020, loss 0.00050\n",
      "running reward: 5.84 at episode 26250, frame count 12180000, epsilon 0.020, loss 0.00130\n",
      "running reward: 5.88 at episode 26260, frame count 12190000, epsilon 0.020, loss 0.00320\n",
      "running reward: 5.96 at episode 26273, frame count 12200000, epsilon 0.020, loss 0.00048\n",
      "running reward: 5.83 at episode 26284, frame count 12210000, epsilon 0.020, loss 0.00052\n",
      "running reward: 5.85 at episode 26296, frame count 12220000, epsilon 0.020, loss 0.00080\n",
      "running reward: 6.13 at episode 26304, frame count 12230000, epsilon 0.020, loss 0.00045\n",
      "running reward: 6.07 at episode 26316, frame count 12240000, epsilon 0.020, loss 0.00035\n",
      "running reward: 6.16 at episode 26326, frame count 12250000, epsilon 0.020, loss 0.00047\n",
      "running reward: 6.12 at episode 26336, frame count 12260000, epsilon 0.020, loss 0.00034\n",
      "running reward: 6.31 at episode 26346, frame count 12270000, epsilon 0.020, loss 0.00136\n",
      "running reward: 6.02 at episode 26357, frame count 12280000, epsilon 0.020, loss 0.00064\n",
      "running reward: 6.33 at episode 26366, frame count 12290000, epsilon 0.020, loss 0.00054\n",
      "running reward: 6.55 at episode 26376, frame count 12300000, epsilon 0.020, loss 0.00216\n",
      "running reward: 6.78 at episode 26387, frame count 12310000, epsilon 0.020, loss 0.00021\n",
      "running reward: 6.58 at episode 26400, frame count 12320000, epsilon 0.020, loss 0.00086\n",
      "running reward: 6.65 at episode 26410, frame count 12330000, epsilon 0.020, loss 0.00039\n",
      "running reward: 6.69 at episode 26420, frame count 12340000, epsilon 0.020, loss 0.00143\n",
      "running reward: 6.75 at episode 26430, frame count 12350000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.62 at episode 26440, frame count 12360000, epsilon 0.020, loss 0.00164\n",
      "running reward: 6.72 at episode 26449, frame count 12370000, epsilon 0.020, loss 0.00317\n",
      "running reward: 6.79 at episode 26460, frame count 12380000, epsilon 0.020, loss 0.00063\n",
      "running reward: 6.28 at episode 26472, frame count 12390000, epsilon 0.020, loss 0.00303\n",
      "running reward: 6.25 at episode 26481, frame count 12400000, epsilon 0.020, loss 0.00183\n",
      "running reward: 6.48 at episode 26490, frame count 12410000, epsilon 0.020, loss 0.00066\n",
      "running reward: 6.38 at episode 26502, frame count 12420000, epsilon 0.020, loss 0.00167\n",
      "running reward: 6.46 at episode 26513, frame count 12430000, epsilon 0.020, loss 0.00237\n",
      "running reward: 6.62 at episode 26521, frame count 12440000, epsilon 0.020, loss 0.00067\n",
      "running reward: 6.69 at episode 26531, frame count 12450000, epsilon 0.020, loss 0.00632\n",
      "running reward: 6.52 at episode 26545, frame count 12460000, epsilon 0.020, loss 0.00132\n",
      "running reward: 6.43 at episode 26555, frame count 12470000, epsilon 0.020, loss 0.00049\n",
      "running reward: 6.64 at episode 26563, frame count 12480000, epsilon 0.020, loss 0.00029\n",
      "running reward: 6.64 at episode 26575, frame count 12490000, epsilon 0.020, loss 0.00025\n",
      "running reward: 6.73 at episode 26584, frame count 12500000, epsilon 0.020, loss 0.00082\n",
      "running reward: 6.73 at episode 26595, frame count 12510000, epsilon 0.020, loss 0.00204\n",
      "running reward: 6.76 at episode 26605, frame count 12520000, epsilon 0.020, loss 0.00877\n",
      "running reward: 7.10 at episode 26613, frame count 12530000, epsilon 0.020, loss 0.00162\n",
      "running reward: 6.80 at episode 26622, frame count 12540000, epsilon 0.020, loss 0.00102\n",
      "running reward: 6.79 at episode 26632, frame count 12550000, epsilon 0.020, loss 0.00906\n",
      "running reward: 6.83 at episode 26642, frame count 12560000, epsilon 0.020, loss 0.00049\n",
      "running reward: 7.08 at episode 26653, frame count 12570000, epsilon 0.020, loss 0.00041\n",
      "running reward: 6.89 at episode 26665, frame count 12580000, epsilon 0.020, loss 0.00051\n",
      "running reward: 6.93 at episode 26675, frame count 12590000, epsilon 0.020, loss 0.00045\n",
      "running reward: 6.84 at episode 26686, frame count 12600000, epsilon 0.020, loss 0.00038\n",
      "running reward: 6.91 at episode 26695, frame count 12610000, epsilon 0.020, loss 0.00059\n",
      "running reward: 7.03 at episode 26705, frame count 12620000, epsilon 0.020, loss 0.00042\n",
      "running reward: 6.84 at episode 26713, frame count 12630000, epsilon 0.020, loss 0.00259\n",
      "running reward: 6.78 at episode 26725, frame count 12640000, epsilon 0.020, loss 0.00031\n",
      "running reward: 6.83 at episode 26735, frame count 12650000, epsilon 0.020, loss 0.00172\n",
      "running reward: 6.95 at episode 26743, frame count 12660000, epsilon 0.020, loss 0.00201\n",
      "running reward: 6.99 at episode 26753, frame count 12670000, epsilon 0.020, loss 0.00052\n",
      "running reward: 6.92 at episode 26765, frame count 12680000, epsilon 0.020, loss 0.00071\n",
      "running reward: 6.92 at episode 26775, frame count 12690000, epsilon 0.020, loss 0.00132\n",
      "running reward: 6.92 at episode 26785, frame count 12700000, epsilon 0.020, loss 0.00039\n",
      "running reward: 6.80 at episode 26795, frame count 12710000, epsilon 0.020, loss 0.00127\n",
      "running reward: 6.78 at episode 26804, frame count 12720000, epsilon 0.020, loss 0.02564\n",
      "running reward: 6.87 at episode 26813, frame count 12730000, epsilon 0.020, loss 0.00030\n",
      "running reward: 6.93 at episode 26823, frame count 12740000, epsilon 0.020, loss 0.00075\n",
      "running reward: 6.79 at episode 26834, frame count 12750000, epsilon 0.020, loss 0.00047\n",
      "running reward: 6.68 at episode 26844, frame count 12760000, epsilon 0.020, loss 0.00118\n",
      "running reward: 6.76 at episode 26851, frame count 12770000, epsilon 0.020, loss 0.00153\n",
      "running reward: 6.72 at episode 26863, frame count 12780000, epsilon 0.020, loss 0.00105\n",
      "running reward: 6.44 at episode 26877, frame count 12790000, epsilon 0.020, loss 0.00096\n",
      "running reward: 6.56 at episode 26887, frame count 12800000, epsilon 0.020, loss 0.00055\n",
      "running reward: 6.41 at episode 26898, frame count 12810000, epsilon 0.020, loss 0.00022\n",
      "running reward: 6.50 at episode 26906, frame count 12820000, epsilon 0.020, loss 0.00046\n",
      "running reward: 6.28 at episode 26920, frame count 12830000, epsilon 0.020, loss 0.00121\n",
      "running reward: 6.04 at episode 26930, frame count 12840000, epsilon 0.020, loss 0.00030\n",
      "running reward: 6.22 at episode 26941, frame count 12850000, epsilon 0.020, loss 0.00062\n",
      "running reward: 5.83 at episode 26953, frame count 12860000, epsilon 0.020, loss 0.00011\n",
      "running reward: 5.90 at episode 26965, frame count 12870000, epsilon 0.020, loss 0.00186\n",
      "running reward: 6.04 at episode 26975, frame count 12880000, epsilon 0.020, loss 0.00115\n",
      "running reward: 5.84 at episode 26988, frame count 12890000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.95 at episode 26999, frame count 12900000, epsilon 0.020, loss 0.00033\n",
      "running reward: 5.75 at episode 27010, frame count 12910000, epsilon 0.020, loss 0.00063\n",
      "running reward: 5.84 at episode 27021, frame count 12920000, epsilon 0.020, loss 0.00080\n",
      "running reward: 5.55 at episode 27035, frame count 12930000, epsilon 0.020, loss 0.02823\n",
      "running reward: 5.61 at episode 27048, frame count 12940000, epsilon 0.020, loss 0.00075\n",
      "running reward: 5.38 at episode 27059, frame count 12950000, epsilon 0.020, loss 0.00030\n",
      "running reward: 5.42 at episode 27069, frame count 12960000, epsilon 0.020, loss 0.00063\n",
      "running reward: 5.08 at episode 27087, frame count 12970000, epsilon 0.020, loss 0.00069\n",
      "running reward: 4.92 at episode 27099, frame count 12980000, epsilon 0.020, loss 0.00035\n",
      "running reward: 4.98 at episode 27113, frame count 12990000, epsilon 0.020, loss 0.00052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 5.15 at episode 27122, frame count 13000000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.29 at episode 27135, frame count 13010000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.05 at episode 27151, frame count 13020000, epsilon 0.020, loss 0.00034\n",
      "running reward: 4.88 at episode 27165, frame count 13030000, epsilon 0.020, loss 0.00057\n",
      "running reward: 4.92 at episode 27178, frame count 13040000, epsilon 0.020, loss 0.00066\n",
      "running reward: 5.05 at episode 27188, frame count 13050000, epsilon 0.020, loss 0.00094\n",
      "running reward: 4.86 at episode 27203, frame count 13060000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.82 at episode 27217, frame count 13070000, epsilon 0.020, loss 0.00035\n",
      "running reward: 4.86 at episode 27228, frame count 13080000, epsilon 0.020, loss 0.00037\n",
      "running reward: 4.46 at episode 27247, frame count 13090000, epsilon 0.020, loss 0.00027\n",
      "running reward: 4.55 at episode 27261, frame count 13100000, epsilon 0.020, loss 0.00026\n",
      "running reward: 4.67 at episode 27275, frame count 13110000, epsilon 0.020, loss 0.00166\n",
      "running reward: 4.22 at episode 27294, frame count 13120000, epsilon 0.020, loss 0.00046\n",
      "running reward: 4.51 at episode 27302, frame count 13130000, epsilon 0.020, loss 0.00055\n",
      "running reward: 4.53 at episode 27313, frame count 13140000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.35 at episode 27329, frame count 13150000, epsilon 0.020, loss 0.00013\n",
      "running reward: 4.67 at episode 27340, frame count 13160000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.07 at episode 27347, frame count 13170000, epsilon 0.020, loss 0.00082\n",
      "running reward: 5.64 at episode 27357, frame count 13180000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.85 at episode 27367, frame count 13190000, epsilon 0.020, loss 0.00077\n",
      "running reward: 6.19 at episode 27377, frame count 13200000, epsilon 0.020, loss 0.00049\n",
      "running reward: 6.23 at episode 27394, frame count 13210000, epsilon 0.020, loss 0.00491\n",
      "running reward: 5.94 at episode 27407, frame count 13220000, epsilon 0.020, loss 0.00050\n",
      "running reward: 6.06 at episode 27418, frame count 13230000, epsilon 0.020, loss 0.02094\n",
      "running reward: 6.11 at episode 27430, frame count 13240000, epsilon 0.020, loss 0.00024\n",
      "running reward: 6.24 at episode 27439, frame count 13250000, epsilon 0.020, loss 0.00207\n",
      "running reward: 6.13 at episode 27450, frame count 13260000, epsilon 0.020, loss 0.00027\n",
      "running reward: 5.48 at episode 27463, frame count 13270000, epsilon 0.020, loss 0.00956\n",
      "running reward: 5.12 at episode 27477, frame count 13280000, epsilon 0.020, loss 0.00151\n",
      "running reward: 5.20 at episode 27491, frame count 13290000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.35 at episode 27504, frame count 13300000, epsilon 0.020, loss 0.00071\n",
      "running reward: 5.49 at episode 27514, frame count 13310000, epsilon 0.020, loss 0.00133\n",
      "running reward: 5.32 at episode 27527, frame count 13320000, epsilon 0.020, loss 0.00098\n",
      "running reward: 5.28 at episode 27537, frame count 13330000, epsilon 0.020, loss 0.02145\n",
      "running reward: 5.43 at episode 27546, frame count 13340000, epsilon 0.020, loss 0.00099\n",
      "running reward: 5.46 at episode 27556, frame count 13350000, epsilon 0.020, loss 0.00046\n",
      "running reward: 5.69 at episode 27567, frame count 13360000, epsilon 0.020, loss 0.00124\n",
      "running reward: 5.90 at episode 27575, frame count 13370000, epsilon 0.020, loss 0.00142\n",
      "running reward: 6.24 at episode 27585, frame count 13380000, epsilon 0.020, loss 0.00108\n",
      "running reward: 6.57 at episode 27597, frame count 13390000, epsilon 0.020, loss 0.00035\n",
      "running reward: 6.22 at episode 27611, frame count 13400000, epsilon 0.020, loss 0.00014\n",
      "running reward: 6.34 at episode 27621, frame count 13410000, epsilon 0.020, loss 0.00051\n",
      "running reward: 6.11 at episode 27636, frame count 13420000, epsilon 0.020, loss 0.00027\n",
      "running reward: 5.90 at episode 27648, frame count 13430000, epsilon 0.020, loss 0.00185\n",
      "running reward: 5.65 at episode 27661, frame count 13440000, epsilon 0.020, loss 0.00098\n",
      "running reward: 5.41 at episode 27673, frame count 13450000, epsilon 0.020, loss 0.00025\n",
      "running reward: 5.37 at episode 27684, frame count 13460000, epsilon 0.020, loss 0.00063\n",
      "running reward: 5.45 at episode 27692, frame count 13470000, epsilon 0.020, loss 0.00147\n",
      "running reward: 5.76 at episode 27701, frame count 13480000, epsilon 0.020, loss 0.00059\n",
      "running reward: 5.73 at episode 27712, frame count 13490000, epsilon 0.020, loss 0.00086\n",
      "running reward: 5.88 at episode 27721, frame count 13500000, epsilon 0.020, loss 0.00115\n",
      "running reward: 6.22 at episode 27731, frame count 13510000, epsilon 0.020, loss 0.00186\n",
      "running reward: 6.39 at episode 27742, frame count 13520000, epsilon 0.020, loss 0.02029\n",
      "running reward: 6.79 at episode 27752, frame count 13530000, epsilon 0.020, loss 0.00033\n",
      "running reward: 6.71 at episode 27761, frame count 13540000, epsilon 0.020, loss 0.00087\n",
      "running reward: 7.19 at episode 27772, frame count 13550000, epsilon 0.020, loss 0.00068\n",
      "running reward: 6.94 at episode 27786, frame count 13560000, epsilon 0.020, loss 0.00599\n",
      "running reward: 6.75 at episode 27795, frame count 13570000, epsilon 0.020, loss 0.00073\n",
      "running reward: 6.64 at episode 27807, frame count 13580000, epsilon 0.020, loss 0.00088\n",
      "running reward: 6.30 at episode 27820, frame count 13590000, epsilon 0.020, loss 0.00227\n",
      "running reward: 6.24 at episode 27832, frame count 13600000, epsilon 0.020, loss 0.00179\n",
      "running reward: 5.98 at episode 27844, frame count 13610000, epsilon 0.020, loss 0.00040\n",
      "running reward: 5.76 at episode 27856, frame count 13620000, epsilon 0.020, loss 0.02586\n",
      "running reward: 5.67 at episode 27865, frame count 13630000, epsilon 0.020, loss 0.00060\n",
      "running reward: 5.61 at episode 27878, frame count 13640000, epsilon 0.020, loss 0.00071\n",
      "running reward: 5.70 at episode 27890, frame count 13650000, epsilon 0.020, loss 0.00033\n",
      "running reward: 5.23 at episode 27906, frame count 13660000, epsilon 0.020, loss 0.00026\n",
      "running reward: 5.27 at episode 27919, frame count 13670000, epsilon 0.020, loss 0.00033\n",
      "running reward: 4.94 at episode 27933, frame count 13680000, epsilon 0.020, loss 0.00134\n",
      "running reward: 5.11 at episode 27942, frame count 13690000, epsilon 0.020, loss 0.00036\n",
      "running reward: 5.01 at episode 27957, frame count 13700000, epsilon 0.020, loss 0.00043\n",
      "running reward: 5.02 at episode 27967, frame count 13710000, epsilon 0.020, loss 0.00066\n",
      "running reward: 5.04 at episode 27980, frame count 13720000, epsilon 0.020, loss 0.00072\n",
      "running reward: 5.21 at episode 27989, frame count 13730000, epsilon 0.020, loss 0.00927\n",
      "running reward: 5.43 at episode 28002, frame count 13740000, epsilon 0.020, loss 0.00092\n",
      "running reward: 5.56 at episode 28011, frame count 13750000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.74 at episode 28022, frame count 13760000, epsilon 0.020, loss 0.00059\n",
      "running reward: 6.24 at episode 28031, frame count 13770000, epsilon 0.020, loss 0.00086\n",
      "running reward: 6.27 at episode 28042, frame count 13780000, epsilon 0.020, loss 0.00017\n",
      "running reward: 6.06 at episode 28053, frame count 13790000, epsilon 0.020, loss 0.00091\n",
      "running reward: 6.71 at episode 28063, frame count 13800000, epsilon 0.020, loss 0.00037\n",
      "running reward: 6.77 at episode 28073, frame count 13810000, epsilon 0.020, loss 0.01722\n",
      "running reward: 6.78 at episode 28085, frame count 13820000, epsilon 0.020, loss 0.00339\n",
      "running reward: 6.94 at episode 28095, frame count 13830000, epsilon 0.020, loss 0.00037\n",
      "running reward: 6.91 at episode 28105, frame count 13840000, epsilon 0.020, loss 0.00075\n",
      "running reward: 6.61 at episode 28116, frame count 13850000, epsilon 0.020, loss 0.00076\n",
      "running reward: 6.60 at episode 28125, frame count 13860000, epsilon 0.020, loss 0.00064\n",
      "running reward: 6.61 at episode 28134, frame count 13870000, epsilon 0.020, loss 0.00267\n",
      "running reward: 6.92 at episode 28143, frame count 13880000, epsilon 0.020, loss 0.00245\n",
      "running reward: 6.32 at episode 28160, frame count 13890000, epsilon 0.020, loss 0.00033\n",
      "running reward: 6.27 at episode 28170, frame count 13900000, epsilon 0.020, loss 0.00096\n",
      "running reward: 6.34 at episode 28178, frame count 13910000, epsilon 0.020, loss 0.00071\n",
      "running reward: 6.41 at episode 28189, frame count 13920000, epsilon 0.020, loss 0.00805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 6.46 at episode 28198, frame count 13930000, epsilon 0.020, loss 0.00083\n",
      "running reward: 6.47 at episode 28208, frame count 13940000, epsilon 0.020, loss 0.00143\n",
      "running reward: 6.53 at episode 28217, frame count 13950000, epsilon 0.020, loss 0.00524\n",
      "running reward: 6.63 at episode 28225, frame count 13960000, epsilon 0.020, loss 0.00252\n",
      "running reward: 6.37 at episode 28236, frame count 13970000, epsilon 0.020, loss 0.00044\n",
      "running reward: 6.43 at episode 28245, frame count 13980000, epsilon 0.020, loss 0.02501\n",
      "running reward: 6.74 at episode 28253, frame count 13990000, epsilon 0.020, loss 0.00026\n",
      "running reward: 6.79 at episode 28265, frame count 14000000, epsilon 0.020, loss 0.00028\n",
      "running reward: 6.59 at episode 28276, frame count 14010000, epsilon 0.020, loss 0.00039\n",
      "running reward: 6.45 at episode 28289, frame count 14020000, epsilon 0.020, loss 0.00046\n",
      "running reward: 6.32 at episode 28299, frame count 14030000, epsilon 0.020, loss 0.00028\n",
      "running reward: 6.18 at episode 28311, frame count 14040000, epsilon 0.020, loss 0.00057\n",
      "running reward: 5.61 at episode 28328, frame count 14050000, epsilon 0.020, loss 0.00017\n",
      "running reward: 5.69 at episode 28338, frame count 14060000, epsilon 0.020, loss 0.00035\n",
      "running reward: 5.13 at episode 28353, frame count 14070000, epsilon 0.020, loss 0.00797\n",
      "running reward: 5.31 at episode 28364, frame count 14080000, epsilon 0.020, loss 0.00081\n",
      "running reward: 5.24 at episode 28376, frame count 14090000, epsilon 0.020, loss 0.00037\n",
      "running reward: 5.27 at episode 28385, frame count 14100000, epsilon 0.020, loss 0.00114\n",
      "running reward: 5.13 at episode 28398, frame count 14110000, epsilon 0.020, loss 0.00872\n",
      "running reward: 5.11 at episode 28409, frame count 14120000, epsilon 0.020, loss 0.00023\n",
      "running reward: 5.17 at episode 28421, frame count 14130000, epsilon 0.020, loss 0.00983\n",
      "running reward: 5.07 at episode 28438, frame count 14140000, epsilon 0.020, loss 0.00028\n",
      "running reward: 5.17 at episode 28452, frame count 14150000, epsilon 0.020, loss 0.00036\n",
      "running reward: 5.06 at episode 28463, frame count 14160000, epsilon 0.020, loss 0.00383\n",
      "running reward: 5.41 at episode 28470, frame count 14170000, epsilon 0.020, loss 0.00553\n",
      "running reward: 5.26 at episode 28483, frame count 14180000, epsilon 0.020, loss 0.00042\n",
      "running reward: 5.26 at episode 28494, frame count 14190000, epsilon 0.020, loss 0.00051\n",
      "running reward: 5.49 at episode 28505, frame count 14200000, epsilon 0.020, loss 0.00073\n",
      "running reward: 5.36 at episode 28517, frame count 14210000, epsilon 0.020, loss 0.00075\n",
      "running reward: 5.41 at episode 28531, frame count 14220000, epsilon 0.020, loss 0.00032\n",
      "running reward: 5.77 at episode 28541, frame count 14230000, epsilon 0.020, loss 0.00034\n",
      "running reward: 5.92 at episode 28553, frame count 14240000, epsilon 0.020, loss 0.00101\n",
      "running reward: 5.34 at episode 28570, frame count 14250000, epsilon 0.020, loss 0.00087\n",
      "running reward: 5.27 at episode 28583, frame count 14260000, epsilon 0.020, loss 0.00021\n",
      "running reward: 5.32 at episode 28594, frame count 14270000, epsilon 0.020, loss 0.00253\n",
      "running reward: 5.09 at episode 28608, frame count 14280000, epsilon 0.020, loss 0.00029\n",
      "running reward: 5.14 at episode 28619, frame count 14290000, epsilon 0.020, loss 0.00122\n",
      "running reward: 4.56 at episode 28639, frame count 14300000, epsilon 0.020, loss 0.00110\n",
      "running reward: 4.32 at episode 28659, frame count 14310000, epsilon 0.020, loss 0.00097\n",
      "running reward: 4.48 at episode 28670, frame count 14320000, epsilon 0.020, loss 0.00056\n",
      "running reward: 4.53 at episode 28680, frame count 14330000, epsilon 0.020, loss 0.00028\n",
      "running reward: 4.55 at episode 28692, frame count 14340000, epsilon 0.020, loss 0.00011\n",
      "running reward: 4.53 at episode 28704, frame count 14350000, epsilon 0.020, loss 0.00065\n",
      "running reward: 4.21 at episode 28721, frame count 14360000, epsilon 0.020, loss 0.00067\n",
      "running reward: 4.75 at episode 28730, frame count 14370000, epsilon 0.020, loss 0.00131\n",
      "running reward: 5.22 at episode 28737, frame count 14380000, epsilon 0.020, loss 0.00033\n",
      "running reward: 5.49 at episode 28749, frame count 14390000, epsilon 0.020, loss 0.00022\n",
      "running reward: 5.90 at episode 28760, frame count 14400000, epsilon 0.020, loss 0.00021\n",
      "running reward: 5.90 at episode 28774, frame count 14410000, epsilon 0.020, loss 0.00379\n",
      "running reward: 5.82 at episode 28788, frame count 14420000, epsilon 0.020, loss 0.00059\n",
      "running reward: 5.76 at episode 28801, frame count 14430000, epsilon 0.020, loss 0.00039\n",
      "running reward: 5.69 at episode 28815, frame count 14440000, epsilon 0.020, loss 0.00180\n",
      "running reward: 6.03 at episode 28825, frame count 14450000, epsilon 0.020, loss 0.00053\n",
      "running reward: 5.60 at episode 28837, frame count 14460000, epsilon 0.020, loss 0.00158\n",
      "running reward: 5.61 at episode 28847, frame count 14470000, epsilon 0.020, loss 0.00042\n",
      "running reward: 5.59 at episode 28858, frame count 14480000, epsilon 0.020, loss 0.00116\n",
      "running reward: 5.34 at episode 28868, frame count 14490000, epsilon 0.020, loss 0.00077\n",
      "running reward: 5.80 at episode 28878, frame count 14500000, epsilon 0.020, loss 0.00136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(state)\n\u001b[1;32m     31\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(state_tensor, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Take best action\u001b[39;00m\n\u001b[1;32m     34\u001b[0m action \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(action_probs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1030\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1033\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py:420\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    403\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py:556\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 556\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, nest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1030\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1033\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/layers/convolutional.py:249\u001b[0m, in \u001b[0;36mConv.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_causal:  \u001b[38;5;66;03m# Apply causal padding to inputs for Conv1D.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mpad(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_causal_padding(inputs))\n\u001b[0;32m--> 249\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convolution_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    252\u001b[0m   output_rank \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:1012\u001b[0m, in \u001b[0;36mconvolution_v2\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.convolution\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvolution_v2\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     dilations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1011\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1012\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvolution_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=redefined-builtin\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:1142\u001b[0m, in \u001b[0;36mconvolution_internal\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name, call_from_convolution, num_spatial_dims)\u001b[0m\n\u001b[1;32m   1139\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     op \u001b[38;5;241m=\u001b[39m conv1d\n\u001b[0;32m-> 1142\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1151\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m channel_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/nn_ops.py:2596\u001b[0m, in \u001b[0;36m_conv2d_expanded_batch\u001b[0;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   2592\u001b[0m input_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m   2593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m input_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m   2594\u001b[0m   \u001b[38;5;66;03m# We avoid calling squeeze_batch_dims to reduce extra python function\u001b[39;00m\n\u001b[1;32m   2595\u001b[0m   \u001b[38;5;66;03m# call slowdown in eager mode.  This branch doesn't require reshapes.\u001b[39;00m\n\u001b[0;32m-> 2596\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdilations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m squeeze_batch_dims(\n\u001b[1;32m   2605\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2606\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2613\u001b[0m     inner_rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   2614\u001b[0m     name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/gen_nn_ops.py:925\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConv2D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrides\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_cudnn_on_gpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cudnn_on_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpadding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexplicit_paddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdilations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    931\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "MAX_EPISODES = 50000\n",
    "\n",
    "while True and episode_count < MAX_EPISODES:  # Run until solved\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        \n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        if frame_count < epsilon_greedy_frames:\n",
    "          epsilon -= epsilon_interval_1 / epsilon_greedy_frames\n",
    "          epsilon = max(epsilon, epsilon_min_1)\n",
    "        \n",
    "        if frame_count > epsilon_greedy_frames and frame_count < 2 * epsilon_greedy_frames:\n",
    "          epsilon -= epsilon_interval_2 / epsilon_greedy_frames\n",
    "          epsilon = max(epsilon, epsilon_min_2)\n",
    "        \n",
    "        if frame_count > 2 * epsilon_greedy_frames:\n",
    "          epsilon -= epsilon_interval_3 / epsilon_greedy_frames\n",
    "          epsilon = max(epsilon, epsilon_min_3)\n",
    "          \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state)\n",
    "        state_next_history.append(state_next)\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every 20th frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices]\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {:.3f}, loss {:.5f}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon, loss))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 18:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        save_model(episode_count)\n",
    "        break\n",
    "        \n",
    "    # save model\n",
    "    if episode_count % 5000 == 0:\n",
    "        save_model(episode_count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
